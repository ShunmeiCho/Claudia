<context>
# 概述
本项目旨在开发一个基于Unitree Go2 R&D Plus平台的智能四足机器人系统**Cluadia**（日语读音：くら），通过深度集成大语言模型技术实现高级自然语言交互功能。R&D Plus版配备足端力传感器和可选深度相机，为系统提供更精确的环境感知和交互能力。系统将具备语音理解、动作执行、自主导航等核心能力，为大学开放日等公共活动提供革新性的交互展示解决方案。

**✅ 官方技术验证状态**（2025年最新确认）：
- **硬件平台**：✅ Unitree Go2 R&D Plus（官方支持，完全匹配PRD需求）
- **开发环境**：✅ Ubuntu 20.04 + ROS2 Foxy（官方测试验证环境）
- **SDK架构**：✅ unitree_sdk2 + unitree_ros2（官方GitHub维护，cyclonedx DDS原生通信）
- **技术优势**：✅ ROS2 DDS直接通信，无需SDK接口转发，提升性能和稳定性
- **工作空间**：✅ cyclonedx_ws官方结构明确，example例程完整
- **集成风险**：🟢 **大幅降低** - 所有核心技术选择均有官方文档支持和验证

**技术发展路径**（当前规划阶段）：
- **第一阶段**：LLM替换遥控器，理解自然语言并调用现有预设动作
- **第二阶段**：LLM自主创建和组合ROS动作序列，实现复杂动作规划
- **第三阶段**：智能导航系统，理解自然语言导航指令并实现自主移动和路径引导
- **后续阶段**：根据研究进展和需求，将持续添加更多高级功能和研究方向

# 核心功能
## 1. 智能语音交互系统

### 第一阶段：预设动作的自然语言触发
- **智能唤醒系统**（类似Hi Siri功能）：
  - **持续监听**：时刻监听环境语音，低功耗待机状态
  - **唤醒词检测**：识别"hi くら"或"hi cluadia"唤醒指令
  - **状态激活**：唤醒后进入交互模式，等待用户语音命令
  - **LED状态指示**：通过前置LED头灯提供直观的状态反馈（与系统默认状态兼容）
    - 🟢 **绿色双闪**：Cluadia唤醒确认（区别于系统单闪）
    - 🔵 **蓝色常亮**：正在处理语音指令（区别于系统蓝色闪烁校准）
    - 🟠 **橙色常亮**：执行动作中（避免与系统黄色低电警告冲突）
    - ⚪ **白色短闪3次**：动作完成确认（区别于系统白色搜索灯常亮）
    - 🔴 **红色三连闪**：Cluadia错误或无法理解（区别于系统单闪/双闪）
    - **待机状态**：保持系统默认绿色常亮（已开机状态），无额外指示
  - **超时管理**：无指令时LED逐渐暗淡并自动返回待机监听状态
- **基本动作指令处理**：
  - **现有预设动作**（已验证API支持）：
    - "お手（握手）" → HELLO(1016) API
    - "お座り（坐下）" → SIT(1009) API
    - "伸展" → STRETCH(1017) API
    - "ダンス（舞蹈）" → DANCE1(1022) 或 DANCE2(1023) API
    - "転圈（转圈）" → 基于EULER(1007) + MOVE(1008) API组合
    - "ジャンプ（跳跃）" → FRONTJUMP(1025) 或 STANDUP(1004) + 运动参数
    - "起立" → STANDUP(1004) API
    - "趴下" → STANDDOWN(1005) API
  - **技术实现**：唤醒后LLM理解日文指令 → API编号映射 → unitree_sdk2py函数调用 → 动作执行（替换原遥控器触发方式）
- **个性化交互响应**：
  - 唤醒后直接接受后续指令：用户说出动作指令后直接执行
  - 对"くらこい"或"くらおいで"的专门呼叫：音源定位、快速接近、友好姿态等连续动作序列
- **自然语言理解**：通过大语言模型解析复杂的口语化指令，支持多种表达方式

### 第二阶段：LLM自主动作创建
- **复合动作规划**：LLM根据复杂指令自主创建动作序列
- **ROS动作生成**：动态生成新的ROS动作节点和控制指令
- **动作组合能力**：将基础动作组合成复杂的行为模式

## 2. 自主导航与路径引导（基于无监督学习）
- **无监督语义导航**：
  - **自主语义发现**：通过多模态传感器数据自动识别和学习环境中的功能区域
  - **交互式语义学习**：结合用户指令"研究棟Aに連れて行って"、"図書館に案内して"等，无监督地建立自然语言与空间位置的映射关系
  - **语义概念自组织**：通过观察人员活动模式和环境特征，自主发现"研究室"、"図書館"、"廊下"等语义标签
- **无监督SLAM建图与定位**：
  - **自组织地图构建**：利用Unitree 4D LiDAR L1的360°×90°全向感知进行无标注环境地图自主构建
  - **特征自动提取**：基于深度学习的无监督特征学习，自动识别环境中的稳定地标和导航节点
  - **拓扑结构发现**：通过探索行为自主发现环境的拓扑连接关系，无需预设地图结构
- **自主探索与路径优化**：
  - **好奇心驱动探索**：基于信息增益最大化的无监督探索策略，主动发现未知区域
  - **无监督避障学习**：基于LiDAR 0.05m超低盲区和30m探测距离，通过试错学习优化避障策略
  - **自适应路径规划**：利用强化学习自主优化路径选择，无需人工设计代价函数

## 3. 多模态感知融合
- **激光雷达感知**：Unitree 4D LiDAR L1提供360°×90°全向环境感知，21600 points/s点云数据
- **视觉感知**：前置高清广角摄像头（1280×720分辨率，120°视角）进行环境识别
- **深度感知**：Intel RealSense D435i深度相机（✅已安装并验证）提供立体视觉和深度信息
  - **安装状态**：✅ 已成功安装，USB 3.2连接，设备序列号346522073864
  - **ROS2集成**：✅ ros-foxy-realsense2-camera (v4.51.1) 已安装并可正常启动
  - **功能验证**：✅ 多传感器识别正常（RGB、深度、红外），6个视频设备可用
  - **技术规格**：深度分辨率1280×720@30fps，最小0.105m深度距离
  - **集成策略**：优先确保4D LiDAR L1功能完整，深度相机作为增强传感器
  - **可用性**：从第一阶段开始即可集成使用，不再是第三阶段限制
- **语音识别**：支持日文语音实时识别和处理（R&D Plus版标配）
- **触觉反馈**：通过足端力传感器（R&D Plus版专有）和本体感觉实现环境适应性
- **惯性导航**：利用LiDAR内置IMU（3轴加速度+3轴陀螺仪）增强定位精度

# 用户体验
## 用户画像
- **目标受众**：所有人群（大学开放日参观者、潜在学生、家长、公众、研究人员、技术展示人员等）

## 关键用户流程

### 第一阶段用户流程（预设动作调用）
1. **智能唤醒交互流程**（主要交互方式）：
   - Cluadia待机状态（持续低功耗监听）→ 用户说"hi くら"或"hi cluadia" → 唤醒确认（如音效或动作） → 用户发出动作指令（如"お手して"）→ 语音识别 → LLM理解意图 → 映射到预设动作 → 调用执行 → 反馈确认 → 自动返回待机状态
2. **直接呼叫交互流程**（特殊指令）：
   - 待机状态 → 用户直接呼叫"くらこい"或"くらおいで" → 立即响应（无需唤醒词）→ 音源定位 → 接近用户 → 执行友好动作 → 等待后续指令或返回待机
3. **连续对话模式**：
   - 唤醒后 → 执行第一个指令 → 短暂等待期（如10秒）→ 用户可继续发出指令（无需重新唤醒）→ 超时后自动返回待机状态

### 第二阶段用户流程（动作创建）
4. **复合动作交互流程**：
   - 用户说"hi くら"或"hi cluadia"唤醒 → 用户发出复杂指令（如"先坐下然后转圈最后握手"）→ LLM解析 → 动作序列规划 → ROS节点生成 → 执行组合动作 → 返回待机
5. **导航引导流程**：
   - 唤醒后用户提出导航需求 → 目标解析 → 路径规划 → 自主移动 → 到达确认 → 返回待机

## UI/UX考虑
- **智能唤醒体验**：
  - **唤醒词设计**：优化针对日语ASR模型的唤醒词方案
    - **主要唤醒词**：「ねえ、くら」（纯日语，推荐）- 最佳ASR识别准确率
    - **备用唤醒词**：「くらこい」（纯日语）- 简洁且独特
    - **兼容唤醒词**：「ハイ、くら」（日语化英语）- 保持"hi"概念但使用片假名
    - **技术优势**：纯日语唤醒词与Parakeet-TDT CTC日语模型完美匹配，提供最高识别精度
  - **LED状态指示**：前置LED头灯作为主要状态反馈，在嘈杂环境中比音频更可靠
  - **待机状态提示**：暗红色LED低亮度持续，表明Cluadia处于监听状态
  - **误触发处理**：智能过滤环境噪音，减少意外唤醒
- **语音输入理解**：
  - 支持自然的日文口语表达（非僵硬命令）
  - 高容错性：处理发音不准、背景噪音、语速变化等情况
  - 支持多种表达方式：`"お手して"`、`"握手してくれる？"`、`"手を出して"`等
- **多层次交互反馈方式**：
  - **第一层（LED视觉反馈）**：与系统默认状态兼容的即时状态指示
    - 绿色双闪：Cluadia唤醒确认，用户可开始说话
    - 蓝色常亮：正在理解语音指令
    - 橙色常亮：正在执行动作
    - 白色短闪3次：动作完成确认
    - 红色三连闪：错误或无法理解
    - **系统状态保留**：绿色常亮（开机）、蓝色闪烁（校准）、黄色闪烁（低电）等系统默认状态不变
  - **第二层（动作反馈）**：机器人动作和姿态变化作为功能性反馈
  - **第三层（音频反馈）**：根据硬件配置可选的语音回应
- **音频输出策略**：
  - **完整交互模式**（启用扬声器）：LED + 语音回应 + 执行动作
    - 适用场景：演示展示、个人交互、宽敞环境
    - 用户体验：完整的多感官沉浸式交互
  - **静音交互模式**（禁用扬声器）：LED + 直接执行动作
    - 适用场景：图书馆、会议室、安静环境、深夜使用
    - 用户体验：高效简洁的视觉交互
  - **动态切换**：可通过语音指令或系统配置实时切换音频模式
- **连续对话体验**：
  - 唤醒后Cluadia状态通过绿色双闪确认，然后保持系统默认绿色常亮
  - 超时前通过动作提示或减少响应敏感度给用户时间提示
  - 连续指令期间LED状态按兼容规则切换，提供实时反馈
- **环境适应性**：
  - LED亮度可根据环境光线自动调节
  - 在强光环境下增强LED对比度
  - 在暗环境中降低LED亮度避免眩光
- **安全性**：确保在人群环境中的安全交互，LED状态帮助周围人员理解机器人当前状态
</context>

<PRD>
# 技术架构
## 目标硬件平台（Unitree Go2 R&D Plus）
- **核心平台**：Unitree Go2 R&D Plus四足机器人（研发增强版）
- **计算单元**：NVIDIA Jetson Orin NX（100 TOPS算力）
- **传感器系统**：
  - **Unitree 4D LiDAR L1**（标配）：
    - 360°×90°全向超广角扫描覆盖
    - 0.05m超低盲区，30m@90%反射率探测距离
    - 21600 points/s有效频率，43200 points/s采样频率
    - ±2.0cm测量精度，>100Klux抗强光能力
    - 内置3轴加速度+3轴陀螺仪IMU
    - 重量230g，尺寸75×75×65mm，功耗6W
    - TTL UART通信接口，Class 1人眼安全等级
  - 前置高清广角摄像头（1280×720，120°视角）
  - **足端力传感器**：R&D Plus版专有配置，提供精确的地面接触反馈
  - **深度相机**：Intel RealSense D435i（选配，待安装）
    - 深度分辨率：1280×720@30fps，848×480@90fps
    - 深度视野：86°×57°（±3°）
    - 最小深度距离：0.105m
    - 尺寸：124×29×26mm
  - 内置麦克风阵列（R&D Plus版支持语音交互）
- **LED照明系统**：
  - 前置LED头灯（搜索灯/前灯，支持多色状态指示）
  - 可编程LED阵列（通过LowCmd消息uint8[12] led字段控制）
  - 支持状态指示：待机、激活、执行、错误等不同模式
- **音频系统配置**：
  - **主要音频配置**：
    - **录音设备**：Unitree Go2内置麦克风阵列（R&D Plus版标配，全向收音）
    - **播放设备**：Unitree Go2内置扬声器系统（主要音频输出）
    - **备用音频**：Jetson Orin NX APE音频处理引擎（开发调试用）
  - **灵活使用策略**：可根据使用环境和场景需求选择启用或禁用音频输出
    - 启用模式：完整语音交互体验（LED + 语音回应 + 动作）
    - 静音模式：纯视觉交互体验（LED + 动作，适合安静环境）
  - **音频控制**：支持运行时动态开启/关闭扬声器输出
  - **音频参数优化**：针对Unitree Go2扬声器进行音频后处理和音量调节

## 软件技术栈（✅ 官方验证配置）
- **操作系统**：Ubuntu 20.04.5 LTS（✅ 官方测试验证环境）
- **机器人框架**：ROS2 Foxy（✅ 官方推荐，与Ubuntu 20.04完美匹配）
- **官方SDK集成**：
  - **unitree_sdk2**：基于cyclonedx实现的核心通信机制（✅ 支持Go2、B2、H1、G1）
  - **unitree_ros2**：ROS2原生集成包（✅ 官方GitHub维护）
  - **技术优势**：ROS2使用DDS作为通讯工具，与机器人底层完全兼容，可直接使用ROS2自带msg进行通讯和控制，无需通过SDK接口转发
- **Unitree Go2 Sport API**：已验证的预设动作控制接口
  - **C++ SDK库**：已完整安装在 `/usr/local/lib/` （libunitree_go2_sdk.a等）
  - **API头文件**：完整接口定义在 `/usr/local/include/unitree/robot/go2/sport/`
  - **预设动作支持**：27个已验证的API接口，包括基础控制、运动控制、高级动作
  - **技术架构方案**：
    - **第一阶段**：LLM + 直接SDK（unitree_sdk2py）→ 快速实现预设动作触发
    - **第二阶段**：LLM + ROS2原生DDS通信（可选升级）→ 复杂动作序列创建
- **大语言模型**：Qwen2.5:7B（本地部署，INT4量化）
- **唤醒词检测**：针对日语优化的轻量级关键词检测系统
  - **主要方案**：PicoVoice Porcupine（推荐）
    - **日语支持**：原生日语唤醒词检测，支持自定义「ねえ、くら」「くらこい」等唤醒词
    - **性能优势**：内存占用<5MB，CPU占用极低，检测延迟<100ms
    - **集成简便**：Python/C++ SDK，ARM64原生支持，完全离线运行
    - **许可模式**：免费开发版，商业部署需要许可证
  - **备选方案**：OpenWakeWord（开源）
    - **适用场景**：需要完全开源解决方案的情况
    - **技术要求**：需要收集日语数据集，手动训练和优化模型
    - **工程量**：中等，需要专门的模型训练和调优工作
  - **技术架构**：唤醒词检测→音频缓冲→Parakeet-TDT ASR→Qwen2.5 LLM
  - **识别精度**：专门针对日语音素优化，与后续ASR模型形成完整技术栈
- **语音识别**：NVIDIA Parakeet-TDT CTC 0.6B日语模型（nvidia/parakeet-tdt_ctc-0.6b-ja）
  - **模型优势**：专为日语优化的轻量级CTC模型，0.6B参数量适合边缘设备
  - **Jetson优化**：支持NVIDIA TensorRT加速，在Orin NX上实现低延迟推理
  - **准确性**：针对日语语音识别进行专门训练和优化
- **通信架构**（✅ 官方DDS架构）：
  - **核心通信**：基于cyclonedx DDS中间件的原生ROS2通信
  - **Jetson ↔ Go2通信**：以太网/USB连接（192.168.123.x网段）
  - **通信协议**：ROS2原生msg直接通信，无需SDK接口转发
  - **通信延迟要求**：< 50ms（控制指令响应）
  - **数据同步**：实时状态反馈和传感器数据流
- **LED控制系统**：
  - 通过unitree_go::msg::LowCmd消息的led字段(uint8[12])控制
  - **兼容性状态机**：在系统默认状态基础上添加Cluadia专用状态指示
    - 保留系统预设：开机、校准、低电量、搜索灯、跟随模式等状态
    - 新增Cluadia状态：通过特殊闪烁模式实现（双闪、三连闪、短闪）
  - 环境光自适应算法：根据摄像头感知的环境亮度调节LED强度，避免与白色搜索灯冲突
- **语音合成**：
  - **TTS引擎**：esnya/japanese_speecht5_tts（144M参数，专为日语优化）
    - 基于SpeechT5架构，使用JVS数据集训练
    - 16维说话人嵌入向量，支持男女声控制
    - 推理速度：16,000 samples/秒，满足实时性要求
    - 多句子分句处理策略，解决静音问题
  - **可配置输出**：支持启用/禁用音频输出的运行时切换
  - **音频优化**：针对Unitree Go2内置扬声器进行音频参数调优和后处理
- **无监督导航系统**（集成YOLO弱监督增强）：
  - **层次化学习架构**：
    - **底层物体感知**：YOLOv8实时物体检测（前置摄像头1280×720@120°）
    - **中层语义推理**：物体检测→环境功能分区推断（弱监督信号）
    - **顶层无监督学习**：基于语义线索的自主导航策略学习
  - **YOLO集成物体检测**：
    - **模型选择**：YOLOv8s（80 FPS，平衡性能与精度）
    - **TensorRT优化**：充分利用Jetson Orin NX GPU加速
    - **物体类别**：人、桌椅、书架、电脑、门、窗户等环境关键要素
    - **计算资源分配**：预留60%算力给无监督学习和LLM，40%给YOLO检测
  - **弱监督语义推理**：
    - **功能区域推断**：桌椅→办公区，书架→图书区，沙发→休息区
    - **语义线索提取**：物体分布模式→环境类型判断
    - **pseudo labels生成**：为无监督算法提供结构化环境信息
  - **无监督SLAM框架**（YOLO增强）：
    - **物体-地标关联**：将YOLO检测的稳定物体作为SLAM地标候选
    - **语义一致性验证**：利用物体检测验证无监督发现的环境特征
    - **多模态特征融合**：点云几何特征 + YOLO语义特征的自监督学习
  - **自主探索引擎**（目标导向）：
    - **语义驱动探索**：优先探索包含目标物体类型的区域
    - **好奇心与目标平衡**：ICM探索 + YOLO目标检测的双重驱动
    - **安全约束**：基于人员检测的动态安全边界调整
  - **强化学习路径规划**（语义增强）：
    - **状态表示增强**：传统几何状态 + YOLO物体语义特征
    - **奖励函数设计**：探索效率 + 语义目标达成 + 安全性的多目标优化
    - **策略泛化能力**：利用物体语义特征提升不同环境间的策略迁移能力
- **仿真环境**：NVIDIA Isaac Sim + unitree_rl_gym

## 系统架构设计
- **唤醒层**：持续低功耗语音监听与关键词检测
- **感知层**：多模态数据采集（语音、视觉、深度感知、4D LiDAR L1全向感知、足端力传感器、IMU惯性数据）
- **理解层**：LLM自然语言处理与场景理解
  - **第一阶段LLM集成流程**：
    ```
    用户语音 → ASR识别 → LLM意图理解 → 动作API映射 → 直接SDK调用 → 动作执行
    ```
  - **第二阶段LLM集成流程**：
    ```
    复杂指令 → LLM解析 → 动作序列规划 → ROS节点生成 → 组合动作执行
    ```
- **决策层**：动作规划与无监督路径规划
  - **无监督决策引擎**：基于强化学习的自主决策系统
  - **探索-利用平衡**：动态平衡环境探索与已知路径利用
  - **多目标优化**：同时考虑效率、安全性、学习价值的决策权衡
- **执行层**：机器人动作控制与移动控制
- **LED控制层**：与系统默认状态兼容的实时状态指示与视觉反馈
  - **兼容性设计原则**：Cluadia应用层LED状态必须与Unitree Go2系统预设状态共存
    - 系统保留状态：红色（系统错误）、绿色常亮（开机）、蓝色闪烁（校准）、黄色闪烁（低电）、白色常亮（搜索灯）、紫色（跟随模式）
    - Cluadia专用状态：通过特殊闪烁模式实现（双闪、三连闪、短闪）
  - 状态映射：将Cluadia功能状态映射到兼容的LED颜色和模式
  - 环境适应：根据环境光线条件自动调节LED亮度，避免干扰系统功能
  - 多模态协调：与音频、动作反馈形成统一的交互体验
- **状态管理层**：系统状态切换（待机↔激活↔执行↔待机）与LED同步
- **故障管理层**：
  - **健康监控**：持续监控关键进程和系统资源状态
  - **故障检测**：实时检测异常情况和性能降级
  - **自动恢复**：故障时的自动重启和优雅降级机制
  - **状态通知**：通过LED和音频向用户通报系统状态
- **热管理层**：
  - **温度监控**：实时监控CPU、GPU、系统环境温度
  - **性能调节**：根据温度动态调整计算负载和推理精度
  - **散热控制**：主动或被动散热策略的协调管理
- **扩展接口层**：为未来功能模块提供标准化集成接口

# 开发环境配置

## 当前开发环境（已完成硬件部署）
### 实际硬件配置（2025年最新检测）
- **计算平台**：NVIDIA Jetson Orin NX Developer Kit（已到货并配置）
- **CPU**：8核ARMv8处理器（目前4核在线，最大1984MHz）
- **内存**：15GB RAM + 7.5GB Swap（大幅超出预期）
- **GPU**：NVIDIA Orin NX（100 TOPS算力，支持CUDA 11.4）
- **系统**：Ubuntu 20.04.5 LTS (aarch64)
- **Tegra版本**：R35.3.1 (2023年3月)
- **网络**：以太网连接 (192.168.123.18/24)

### 音频系统配置
- **录音设备**：Unitree Go2内置麦克风阵列（R&D Plus版标配）
- **播放设备**：Unitree Go2内置扬声器系统
- **备用音频**：Jetson Orin NX内置APE音频处理引擎（备用录音支持）

### 软件环境现状（✅ 官方架构对齐）
- **Python**：3.8.10（Ubuntu 20.04标准版本）
- **ROS2**：Foxy（已安装，Ubuntu 20.04标准配置，✅ 官方测试验证环境）
- **CUDA**：11.4完整工具链（已安装）
- **深度学习框架**：待安装优化
- **Unitree SDK集成状态**：
  - **官方工作空间结构**（基于官方unitree_ros2仓库）：
    - **cyclonedx_ws/**：编译和安装Unitree机器人ROS2 msg的工作空间
      - `cyclonedx_ws/unitree/unitree_go/`：Go2状态获取和控制相关的ROS2 msg定义
      - `cyclonedx_ws/unitree/unitree_api/`：API接口相关的ROS2 msg定义
    - **example/**：Unitree机器人ROS2相关例程和示例代码
  - **当前安装状态**：C++ SDK已安装，Python SDK和ROS2集成待配置

### Unitree Go2集成状态
- **机器人状态**：硬件已到货并可操作，C++ SDK已安装
- **硬件验证状态**：
  - ✅ **遥控器控制**：已确认可通过遥控器正常控制Claudia执行各种动作
  - ✅ **基础功能**：机器人本体、传感器、音频系统均工作正常
  - ✅ **动作系统**：预设动作通过遥控器触发验证可用
- **已发现SDK安装**：
  - ✅ **C++ SDK库**：已完整安装（/usr/local/lib/libunitree_go2_sdk.a等）
  - ✅ **API头文件**：完整接口定义（/usr/local/include/unitree/robot/go2/sport/）
  - ✅ **预设动作API**：27个可用接口已验证（sport_api.hpp）
  - ✅ **系统用户配置**：unitree用户账户已存在
  - ✅ **DDS中间件**：cyclonedx_ws工作空间（支持通信）
- **待完成任务**：
  - 安装unitree_sdk2py Python接口（当前缺失的关键组件）
  - 建立编程式机器人通信接口和测试连接
  - 验证通过代码调用基础动作功能
  - 验证传感器系统数据获取

## 开发策略

### 第一阶段：系统环境优化与Unitree集成
**目标**：在当前Jetson Orin NX环境完成系统优化和机器人硬件集成
- **系统环境优化**：
  - ROS2保持Foxy版本（Ubuntu 20.04的标准配置）
  - Ubuntu系统保持20.04.5 LTS（稳定且已验证）
  - Python环境优化和深度学习框架安装
- **Unitree Go2集成**：
  - 安装unitree_ros2和unitree_sdk2完整包（当前仅有C++ SDK库）
  - 安装unitree_sdk2py Python接口（关键缺失组件）
  - 配置机器人通信接口（以太网/USB）
  - 验证基本动作控制功能
  - 确认4D LiDAR L1和摄像头集成状态
- **音频系统验证**：
  - 测试Jetson内置APE音频系统
  - 验证Unitree机器人自带音频设备
  - 配置最优音频输入输出方案

### 第二阶段：核心AI算法部署（第一阶段功能实现）
**目标**：实现LLM替换遥控器的自然语言动作触发
- **完整语音交互链路**：部署端到端日语语音处理系统
  - **唤醒词检测**：集成Porcupine日语唤醒词检测（「ねえ、くら」「くらこい」）
  - **语音识别**：部署NVIDIA Parakeet-TDT CTC 0.6B日语模型
  - **语音合成**：集成esnya/japanese_speecht5_tts日语TTS系统
  - **链路优化**：优化唤醒→ASR→LLM→TTS→动作的完整处理延迟
- **大语言模型部署**：Qwen2.5-7B量化版本（利用15GB RAM优势）
- **预设动作映射系统**：
  - 分析现有Unitree预设动作库
  - 建立自然语言指令到动作函数的映射关系
  - 实现LLM理解→动作调用的完整链路
- **个性化响应机制**：开发"くら"和"くらこい/おいで"呼叫响应功能
- **端到端验证**：语音→理解→预设动作执行的完整流程测试

### 第三阶段：LLM自主动作创建与高级功能开发
**目标**：实现LLM自主动作规划和完整感知导航功能
- **LLM动作创建系统**（第二阶段功能）：
  - 复合动作指令解析与规划
  - 动态ROS动作节点生成
  - 基础动作的智能组合与序列化
- **多模态感知融合**：4D LiDAR + 摄像头数据融合
- **SLAM建图与导航**：环境地图构建和自主移动
- **高级交互能力**：复杂语音指令理解和动作序列执行

# 开发路线图
## 第一阶段：系统环境优化与基础集成（当前优先）
- **系统基线测试**（新增）：
  - **网络通信验证**：测试Jetson与Go2之间的以太网/USB通信
  - **基础硬件功能测试**：验证所有传感器和执行器的基本功能
  - **热管理基线建立**：测量系统在不同负载下的温度表现
  - **内存和性能基准**：建立系统资源使用的基准数据
- **ROS2环境配置（✅ 官方验证路径）**：
  - 优化当前Foxy环境（Ubuntu 20.04 + ROS2 Foxy官方测试组合）
  - 配置开发工作空间，为Unitree集成做准备
  - 验证cyclonedx DDS中间件环境
- **Unitree SDK安装（基于官方最佳实践）**：
  - **第一步：克隆官方仓库**
    ```bash
    git clone https://github.com/unitreerobotics/unitree_ros2
    git clone https://github.com/unitreerobotics/unitree_sdk2
    ```
  - **第二步：配置cyclonedx工作空间**
    - 编译cyclonedx_ws工作空间（ROS2 msg定义）
    - 安装unitree_go和unitree_api消息包
    - 验证ROS2原生DDS通信环境
  - **第三步：Python SDK集成**
    - 安装unitree_sdk2py Python接口（当前关键缺失组件）
    - 建立Python与C++ SDK库的连接
    - 验证基础通信和动作调用功能
  - **第四步：example例程验证**
    - 运行官方提供的ROS2例程
    - 验证机器人通信接口和基础控制功能
- **硬件验证**：确认Go2基本功能、传感器状态、音频配置
- **深度相机集成**：✅ Intel RealSense D435i深度相机已安装并验证，集成深度数据采集和处理流程
- **Python环境优化**：配置Python 3.8.10开发环境，安装深度学习框架
- **CUDA环境验证**：确认GPU加速和推理环境配置
- **音频系统验证**：测试Unitree Go2内置扬声器和麦克风阵列，配置最优音频参数
- **唤醒词检测系统**：
  - **Porcupine集成**：安装PicoVoice Porcupine Python SDK，验证基础功能
  - **日语唤醒词训练**：通过Picovoice Console创建「ねえ、くら」「くらこい」自定义模型
  - **性能基准测试**：测试唤醒词检测延迟、准确率和资源占用
  - **噪音环境测试**：验证在不同噪音环境下的检测鲁棒性

## 第二阶段：LLM替换遥控器功能实现（第一阶段功能）
- **unitree_sdk2py安装与配置（✅ 基于官方架构）**：
  - **Python SDK集成**：安装unitree_sdk2py接口，建立与cyclonedx DDS的Python连接
  - **ROS2原生通信验证**：确认Python可直接使用ROS2 msg进行机器人通信，无需SDK接口转发
  - **基础功能验证**：测试通过Python直接调用预设动作API
  - **官方example运行**：验证官方ROS2例程的Python集成运行
- **LED兼容控制系统开发**：
  - 实现LowCmd消息LED字段(uint8[12])的Python接口封装
  - 开发与系统默认状态兼容的LED状态管理器
  - 研究系统预设LED模式，确保Cluadia状态不与默认行为冲突
  - 集成环境光感知：利用前置摄像头检测环境亮度并自动调节LED强度
  - 实现LED特殊闪烁模式（双闪、三连闪、短闪），区别于系统默认模式
- **智能唤醒系统开发**：
  - 部署轻量级关键词检测模型（支持"hi くら"、"hi cluadia"）
  - 实现低功耗持续监听机制
  - 开发状态管理系统（待机↔激活↔执行状态转换）
  - 集成LED唤醒确认反馈（绿色双闪）和兼容状态指示系统
  - 实现连续对话模式和非LED超时提示机制
- **日文ASR模型部署**：NVIDIA Parakeet-TDT CTC 0.6B日语模型在Jetson Orin NX上的部署和优化
  - 模型量化和TensorRT优化，实现实时语音识别
  - 音频预处理管道配置，适配Unitree Go2麦克风阵列
  - 延迟性能测试和参数调优，确保< 500ms识别延迟
- **LLM量化部署**：Qwen2.5-7B模型利用15GB RAM优势进行优化部署
- **✅ 预设动作库分析**：已完成 - 发现并验证27个可用的Go2 Sport API接口
  - 基础控制动作：DAMP(1001), BALANCESTAND(1002), STANDUP(1004), STANDDOWN(1005)等
  - 运动控制动作：MOVE(1008), SIT(1009), RISESIT(1010), EULER(1007)等  
  - 高级动作：HELLO(1016), STRETCH(1017), DANCE1(1022), DANCE2(1023)等
  - C++ SDK完整安装确认：/usr/local/include/unitree/robot/go2/sport/sport_api.hpp
- **自然语言映射系统**：
  - 建立日文指令到预设动作API的映射表
    - "お手" → HELLO(1016)
    - "お座り" → SIT(1009)
    - "伸展" → STRETCH(1017)
    - "サークル/転圈" → 基于EULER控制的旋转动作
    - "ジャンプ" → 组合STANDUP + 特殊运动参数
    - "ダンス" → DANCE1(1022) 或 DANCE2(1023)
  - 实现LLM意图识别到unitree_sdk2py API调用的转换
  - 替换原有遥控器触发方式为Python函数直接调用
  - 集成动作执行期间的LED状态指示（橙色执行中，白色完成）
- **个性化响应开发**：开发"くらこい/おいで"直接呼叫响应功能（无需唤醒词）
- **日语TTS系统开发**：
  - 部署esnya/japanese_speecht5_tts模型在Jetson Orin NX上
  - 集成SpeechT5FeatureExtractor和SpeechT5Processor
  - 实现多句子分句处理，解决静音问题
  - 配置16维说话人嵌入向量，支持声音个性化
  - 与HiFiGAN vocoder集成，确保音质
- **多模态反馈集成**：LED视觉反馈与动作反馈的协调，确保用户能清晰理解机器人当前状态
- **端到端流程验证**：「ねえ、くら」唤醒→LED绿色双闪确认→语音输入→LED蓝色常亮处理→ASR→LLM意图理解→LED橙色常亮执行→预设动作调用→LED白色短闪3次完成→返回系统默认绿色常亮的完整测试
- **性能优化与压力测试**（新增）：
  - **长时间运行稳定性测试**：连续运行1.5小时以上，监控系统稳定性
  - **热管理效果验证**：验证热管理策略在高负载下的有效性
  - **内存泄漏检测和修复**：识别并修复长期运行中的内存泄漏问题
  - **并发负载测试**：验证ASR、LLM、TTS、机器人控制的并发性能
  - **故障恢复测试**：模拟各种故障场景，验证自动恢复机制

## 第三阶段：LLM自主动作创建与YOLO增强无监督智能导航（第二阶段功能实现）
- **LLM动作创建系统**：
  - 复杂指令解析：理解组合动作指令（如"先坐下然后转圈最后握手"）
  - 动作序列规划：将复杂指令分解为可执行的动作步骤
  - 动态ROS节点生成：LLM自主创建新的ROS动作控制节点
  - 基础动作组合：智能组合现有预设动作形成新的行为模式
- **多模态传感器系统集成**（双摄像头YOLO视觉增强）：
  - **4D LiDAR L1点云处理**：21600 points/s有效频率，几何SLAM基础
  - **双摄像头YOLO系统**：
    - **主要检测摄像头**：Unitree Go2本体摄像头（1280×720@120°广角），专用于YOLO物体检测和导航
    - **深度增强摄像头**：Intel RealSense D435i（1920×1080@69°视角），提供3D定位和高分辨率备份
    - **检测融合策略**：主摄像头YOLO检测 + 深度相机3D定位增强
    - **故障容错机制**：主摄像头故障时自动切换到RealSense RGB检测
  - **3D物体定位系统**：结合YOLO 2D检测和RealSense深度信息实现精确3D空间定位
  - **足端力传感器数据**：R&D Plus版专有，提供地面接触和材质信息
  - **LiDAR内置IMU数据**：3轴加速度+3轴陀螺仪，姿态估计与运动规划
  - **多模态数据融合**：几何（LiDAR）+ 语义（双摄像头YOLO）+ 深度（RealSense）+ 触觉（足端力）的协同处理
- **YOLO物体检测系统集成**（双摄像头配置）：
  - **YOLOv8s模型部署**：在Jetson Orin NX上部署TensorRT优化的YOLOv8s模型（主摄像头目标60 FPS，GPU占用25%）
  - **主要检测管道**：Unitree本体摄像头(1280×720@120°)→预处理→YOLO推理→语义标注
  - **深度增强管道**：RealSense RGB(1920×1080@30fps)→高分辨率补充检测→3D定位融合
  - **双摄像头协同策略**：
    - **主摄像头负责**：实时导航检测、人员安全检测、大范围环境感知
    - **深度摄像头负责**：精确3D定位、细节物体识别、主摄像头故障备份
    - **检测结果融合**：空间坐标对齐、重复检测去除、置信度加权融合
  - **目标物体类别定制**：针对大学环境优化检测类别（人员、桌椅、书架、电脑、门、标识牌等）
  - **自适应检测模式**：根据任务需求动态调整主/备摄像头使用策略
  - **检测结果稳定化系统**：置信度阈值、时间一致性检查、多摄像头交叉验证
- **弱监督语义推理引擎**：
  - **物体-功能区域映射**：建立物体组合模式与环境功能的对应关系
  - **语义线索提取算法**：从物体分布推断空间功能（办公区、图书区、会议室、走廊等）
  - **pseudo labels生成**：为无监督学习算法生成结构化的环境语义信息
  - **置信度评估机制**：评估语义推理结果的可靠性，过滤低质量pseudo labels
- **无监督环境感知与自组织建图**（YOLO增强）：
  - **多模态特征融合**：LiDAR几何特征 + YOLO语义特征的联合学习
  - **语义一致性SLAM**：利用物体检测验证和增强无监督SLAM的地标识别
  - **拓扑地图自构建**：结合物体语义信息优化环境拓扑结构发现
  - **物体-地标关联学习**：将稳定检测的物体转化为可靠的SLAM导航地标
- **无监督语义导航系统**：
  - **交互式语义学习**：通过用户指令历史无监督学习自然语言与空间位置的映射
  - **功能区域自发现**：利用人员活动模式和多模态数据自动识别环境功能分区
  - **语义概念演化**：持续学习和优化语义理解，适应环境变化
- **强化学习路径规划**：
  - **好奇心驱动探索**：基于ICM模块的自主环境探索策略
  - **多目标路径优化**：同时优化探索效率、导航安全性和用户体验
  - **自适应策略学习**：根据不同场景自动调整导航行为
- **无监督避障与安全控制**：
  - **动态障碍物学习**：通过观察自主学习不同类型障碍物的运动模式
  - **安全边界自发现**：基于试错学习自动优化安全距离和速度参数
  - **紧急情况自处理**：无监督学习识别和应对异常情况
- **多模态无监督融合**：
  - **跨模态自监督学习**：视觉、深度、语音、激光雷达、触觉数据的无监督融合学习
  - **特征对齐与补偿**：自动学习不同传感器间的特征关联和互补关系
  - **传感器故障自适应**：当某个传感器失效时自动调整融合策略

## 产品化阶段（第四阶段）
- **性能优化**：提升响应速度和识别准确率
- **展示脚本开发**：为不同场景准备标准化演示流程
- **系统稳定性测试**：长时间运行与错误恢复机制
- **用户培训材料**：操作手册与演示指导

## 未来扩展阶段（第五阶段及后续）
- **功能模块化**：为后续新功能集成提供架构基础
- **研究方向拓展**：根据实际应用反馈和技术发展趋势，识别并集成新的研究方向
- **能力持续演进**：保持系统的可扩展性，支持未来功能的无缝添加
- **技术路线图更新**：定期评估和更新技术发展路径

# 逻辑依赖链（基于实际环境）
## 第一阶段核心依赖
1. **ROS2环境优化** → **Foxy工作空间配置** → **开发环境稳定性**
2. **Unitree SDK安装** → **机器人通信建立** → **基础控制验证**
3. **硬件集成验证** → **传感器状态确认** → **音频系统配置**
4. **Python环境配置** → **深度学习框架部署** → **AI模型运行环境**

## 第二阶段核心功能依赖
1. **音频系统集成** → **唤醒词检测模型部署** → **智能唤醒功能验证**
2. **唤醒系统建立** → **NVIDIA Parakeet-TDT模型部署** → **日语语音识别准确性验证**
3. **TTS系统集成** → **esnya/japanese_speecht5_tts部署** → **日语语音合成质量验证**
4. **LLM优化部署** → **实时推理性能** → **指令理解能力验证**
5. **机器人控制接口** → **动作指令执行** → **真实动作反馈**
6. **状态管理系统** → **完整唤醒交互流程** → **端到端性能基准建立**

## 第三阶段高级功能依赖
1. **传感器数据处理** → **SLAM建图能力** → **环境理解与导航**
2. **多模态融合** → **智能决策系统** → **复杂交互响应**
3. **导航系统集成** → **路径规划优化** → **安全移动控制**
4. **系统集成优化** → **稳定性验证** → **展示就绪状态**

# 风险与缓解策略
## 双摄像头YOLO配置技术决策总结

### 🎯 **最终采用方案：主摄像头优先 + 深度增强**

**核心配置：**
- **主要检测**：Unitree Go2本体摄像头（1280×720@120°广角）→ YOLO物体检测
- **深度增强**：Intel RealSense D435i深度信息 → 3D空间定位
- **备份检测**：RealSense RGB流（1920×1080@69°）→ 故障恢复
- **计算分配**：25% GPU主检测，60% GPU用于LLM+导航，15% GPU预留

**技术优势：**
- ✅ **视角最优**：120°广角覆盖导航需求，69°提供精确细节
- ✅ **资源高效**：避免双摄像头并行的计算资源浪费
- ✅ **系统稳定**：使用机器人原生摄像头确保最佳兼容性
- ✅ **功能互补**：广角导航 + 精确3D定位 + 高分辨率备份
- ✅ **故障容错**：主摄像头失效时自动切换到高质量备份方案

**实现架构：**
```
物体检测管道：
├── 主要路径：Unitree摄像头 → YOLOv8s → 2D检测框
├── 深度增强：RealSense深度 → 3D坐标计算  
├── 结果融合：2D+3D → 完整物体信息
└── 备份路径：RealSense RGB → 故障恢复YOLO检测
```

**弱监督学习集成：**
- **语义线索源**：主摄像头YOLO检测结果作为pseudo labels
- **空间理解增强**：深度信息提升物体-环境关联学习
- **学习数据质量**：多摄像头交叉验证提升训练数据可靠性

---

## 技术挑战
- **语音识别精度**：在嘈杂环境下可能识别错误
  - *缓解措施*：采用降噪算法，实现多麦克风阵列波束成形
  - *唤醒词层面*：Porcupine日语唤醒词检测作为第一道准确过滤
- **LLM理解泛化**：对超出训练范围的指令可能误解
  - *缓解措施*：限定指令范围，优化prompt工程，实现意图置信度评估
- **无监督学习特有挑战**：
  - **学习收敛性风险**：无监督算法可能陷入局部最优或发散
    - *缓解措施*：多种初始化策略，正则化约束，学习进度监控和干预机制
  - **错误概念学习风险**：可能学到错误的语义映射或危险的导航策略
    - *缓解措施*：安全约束学习，人类反馈集成，关键决策人工验证机制
  - **探索安全性风险**：好奇心驱动探索可能导致危险行为
    - *缓解措施*：安全边界硬约束，保守探索策略，实时风险评估系统
  - **学习数据质量风险**：在有限展示环境中学习数据可能不足或偏态
    - *缓解措施*：预训练基础模型，数据增强技术，小样本学习优化
- **Sim2Real差异**：仿真训练结果在实机上表现不佳
  - *缓解措施*：采用域随机化，增加实机测试时间
- **热管理风险**：长时间运行LLM和无监督学习算法可能导致过热降频
  - *缓解措施*：实时温度监控，过热时切换轻量级模型，考虑主动散热方案
- **网络依赖风险**：部分组件可能需要网络连接进行模型更新
  - *缓解措施*：确保核心功能完全离线运行，网络仅用于非关键更新
- **用户安全风险**：四足机器人在人群密集环境中的安全隐患，特别是学习阶段的不可预测行为
  - *缓解措施*：保守运动参数设置，实现紧急停止功能，人员检测与避让算法，学习过程的安全监督

## 资源约束
- **软件环境配置（✅ 官方验证降低风险）**：
  - 当前：Ubuntu 20.04 + ROS2 Foxy（✅ 官方测试验证环境）
  - 目标：保持Ubuntu 20.04 + ROS2 Foxy（✅ 官方推荐稳定配置）
  - *缓解措施*：采用官方测试验证的环境组合，避免兼容性问题，确保与Unitree SDK完美兼容
- **内存管理风险**：接近15GB RAM上限时的系统稳定性
  - *缓解措施*：实时监控RAM和Swap使用率，内存不足时禁用非关键功能，定期清理模型缓存
- **开发时间限制**：项目交付周期紧张
  - *缓解措施*：利用硬件已到货优势，直接开发，优先核心功能
- **SDK集成兼容性（✅ 风险大幅降低）**：
  - **原风险评估**：Unitree SDK与当前环境兼容性不确定
  - **官方验证状态**：✅ Ubuntu 20.04 + ROS2 Foxy为官方测试验证环境
  - **技术优势确认**：✅ ROS2 DDS原生通信，无需SDK接口转发
  - **工作空间结构明确**：✅ cyclonedx_ws和example结构已有官方定义
  - *缓解措施*：严格按照官方文档配置环境，使用官方GitHub最新版本，定期同步官方更新
- **音频输出策略选择**：不同使用场景下的音频模式管理
  - *缓解措施*：实现音频输出的动态开关机制，提供明确的使用场景指导
- **深度相机集成风险**：Intel RealSense D435i与现有传感器系统的兼容性
  - *缓解措施*：明确深度相机必要性等级，提供无深度相机降级方案，优先验证核心传感器功能
- **双摄像头配置特有风险**：
  - **摄像头同步风险**：两个摄像头数据时序不一致可能导致检测结果错位
    - *缓解措施*：实现时间戳对齐机制，设置合理的数据融合窗口，建立异步处理容错
  - **空间标定复杂性**：两个摄像头的空间坐标系对齐挑战
    - *缓解措施*：建立摄像头间标定流程，实现自动标定验证，提供手动标定备份方案
  - **计算资源竞争**：两个摄像头数据处理可能导致GPU内存竞争
    - *缓解措施*：实现智能调度机制，动态调整处理优先级，GPU内存不足时自动降级到单摄像头模式
  - **检测一致性风险**：不同摄像头对同一物体的检测结果可能存在差异
    - *缓解措施*：建立检测结果置信度评估，实现多摄像头投票机制，设置异常检测过滤

## 安全考虑
- **人机交互安全**：避免在演示中造成人员伤害
  - *缓解措施*：保守的运动参数设置，安全边界控制，紧急停止功能
- **系统稳定性**：防止演示过程中系统崩溃
  - *缓解措施*：充分测试，故障检测与恢复机制
- **数据安全和隐私**：保护用户语音数据和交互隐私
  - *缓解措施*：所有语音数据仅在设备本地处理，交互结束后清除音频缓存，限制数据访问权限

## 故障恢复机制
- **系统监控**：
  - **看门狗监控**：监控关键进程（ASR、LLM、TTS、机器人控制）健康状态
  - **资源监控**：实时监控CPU、GPU、内存、温度等系统资源
  - **网络监控**：监控Jetson与Go2之间的通信状态
- **自动恢复策略**：
  - **进程重启**：关键进程崩溃后5秒内自动重启
  - **安全模式**：关键组件失效时保持最小功能（基础LED状态指示）
  - **状态保持**：通过LED和音频提示告知用户系统状态和错误信息
  - **优雅降级**：资源不足时按优先级关闭非关键功能（深度相机→TTS→复杂LLM推理）
- **手动恢复接口**：
  - **紧急停止**：物理按键或语音指令立即停止所有动作
  - **系统重启**：远程或本地重启整个AI系统而不影响机器人基础功能

# 附录
## 系统环境详细记录（2025年更新）
### 硬件规格确认
```bash
# 系统信息
uname -a: Linux ubuntu 5.10.104-tegra #1 SMP PREEMPT Sun Mar 19 07:55:28 PDT 2023 aarch64 aarch64 aarch64 GNU/Linux
设备型号: NVIDIA Orin NX Developer Kit
Tegra版本: R35.3.1 (2023年3月)

# CPU配置
架构: aarch64 (ARMv8)
CPU核心: 8核（目前4核在线）
最大频率: 1984MHz
缓存: L1d=256KB, L1i=256KB, L2=1MB, L3=2MB

# 内存配置
物理内存: 15GB RAM
交换空间: 7.5GB Swap
总可用: ~22.5GB

# GPU与CUDA
GPU: NVIDIA Orin NX (100 TOPS)
CUDA版本: 11.4完整工具链
支持架构: arm64
```

### 软件环境记录
```bash
# 操作系统
系统: Ubuntu 20.04.5 LTS
内核: 5.10.104-tegra
架构: aarch64

# 开发环境
Python: 3.8.10
pip: 20.0.2
ROS2: Foxy (Ubuntu 20.04标准配置)
Docker: 已安装

# 音频系统
主要音频: Unitree Go2内置扬声器+麦克风阵列
备用音频: Jetson APE (多通道), HDMI 0-3
```

### 网络配置
```bash
主网络接口: eth0 (192.168.123.18/24)
Docker网络: docker0 (172.17.0.1/16)
```

## Unitree Go2预设动作API完整列表（已验证）
基于`/usr/local/include/unitree/robot/go2/sport/sport_api.hpp`的API接口：

### **基础控制动作**
- **DAMP** (1001) - 阻尼模式/停止
- **BALANCESTAND** (1002) - 平衡站立  
- **STOPMOVE** (1003) - 停止移动
- **STANDUP** (1004) - 站起来
- **STANDDOWN** (1005) - 趴下
- **RECOVERYSTAND** (1006) - 恢复站立（摔倒后站起）

### **运动控制动作**  
- **EULER** (1007) - 欧拉角姿态控制
- **MOVE** (1008) - 移动控制（vx, vy, vyaw参数）
- **SIT** (1009) - 坐下 *→ 对应PRD中的"お座り"*
- **RISESIT** (1010) - 从坐姿站起

### **高级动作**
- **HELLO** (1016) - 握手/招手动作 *→ 对应PRD中的"お手"*
- **STRETCH** (1017) - 伸展动作 *→ 对应PRD中的"伸展"*
- **DANCE1** (1022) - 舞蹈动作1 *→ 可映射到"ダンス"*
- **DANCE2** (1023) - 舞蹈动作2 *→ 可映射到"サークル"*

### **扩展动作**
- **FRONTFLIP** (1024) - 前空翻
- **FRONTJUMP** (1025) - 前跳
- **FRONTPOUNCE** (1026) - 前扑

### **技术实现说明**
- **API调用方式**：通过unitree_sdk2py Python SDK调用对应编号的动作函数
- **LLM映射策略**：日文指令 → LLM意图识别 → API编号映射 → SDK函数调用
- **组合动作可能性**：基础API可组合实现复杂动作（如"转圈"可通过EULER + MOVE实现）

## 参考技术文档  
- Unitree Go2官方文档与SDK
- ROS2 Foxy开发指南
- NVIDIA Jetson Orin NX技术规格
- Qwen2.5模型部署指南

## 硬件接口验证附录（已验证的SDK接口）

基于实际系统检测，以下是在`/usr/local/include/unitree/go2_idl/`中发现的完整硬件接口定义：

### **LED控制接口**（已验证）
- **文件位置**：`/usr/local/include/unitree/go2_idl/LowCmd_.hpp`
- **接口定义**：
  ```cpp
  std::array<uint8_t, 12> led_ = { };  // 第34行
  ```
- **功能说明**：12个LED灯的独立控制，支持RGB颜色和亮度调节
- **PRD对应**：完全匹配PRD中描述的"uint8[12] led字段"LED控制系统

### **IMU惯性传感器接口**（已验证）
- **文件位置**：`/usr/local/include/unitree/go2_idl/IMUState_.hpp`
- **接口定义**：
  ```cpp
  std::array<float, 4> quaternion_ = { };     // 四元数
  std::array<float, 3> gyroscope_ = { };      // 陀螺仪 (rad/s)
  std::array<float, 3> accelerometer_ = { };  // 加速度计 (m/s²)
  std::array<float, 3> rpy_ = { };            // RPY角度 (roll/pitch/yaw)
  uint8_t temperature_ = 0;                   // 温度传感器
  ```
- **功能说明**：完整的6轴IMU数据，包含姿态、角速度、线性加速度
- **PRD对应**：完全匹配PRD中描述的IMU惯性传感器数据结构

### **4D LiDAR L1接口**（已验证）
- **文件位置**：`/usr/local/include/unitree/go2_idl/LidarState_.hpp`
- **接口定义**：
  ```cpp
  std::array<float, 2> stamp_ = { };          // 时间戳
  std::string firmware_version_ = "";         // 固件版本
  std::string software_version_ = "";         // 软件版本  
  std::string sdk_version_ = "";              // SDK版本
  uint8_t sys_rotation_speed_ = 0;            // 系统旋转速度
  uint8_t com_rotation_speed_ = 0;            // 通信旋转速度
  float error_state_ = 0.0f;                  // 错误状态
  std::array<float, 3> imu_temp_ = { };       // IMU温度
  uint8_t progress_ = 0;                      // 进度状态
  ```
- **功能说明**：LiDAR系统状态监测，支持360°×90°全向扫描
- **PRD对应**：完全匹配PRD中描述的"21600 points/s有效频率"LiDAR系统

### **音频数据接口**（已验证）
- **文件位置**：`/usr/local/include/unitree/go2_idl/AudioData_.hpp`
- **接口定义**：
  ```cpp
  std::vector<uint8_t> data_ = { };           // 音频数据流
  uint64_t time_frame_ = 0;                   // 时间帧
  ```
- **功能说明**：支持音频数据的实时采集和处理
- **PRD对应**：完全匹配PRD中描述的"内置麦克风阵列"和"音频输出系统"

### **运动状态接口**（已验证）  
- **文件位置**：`/usr/local/include/unitree/go2_idl/LowState_.hpp`
- **核心接口定义**：
  ```cpp
  std::array<MotorState, 20> motor_state_ = { };  // 20个电机状态
  IMUState imu_state_ = { };                      // IMU状态数据
  std::array<uint8_t, 40> wireless_remote_ = { }; // 无线遥控器数据
  uint32_t crc_ = 0;                              // 数据校验
  ```
- **功能说明**：完整的机器人状态反馈，包含关节、IMU、遥控器数据
- **PRD对应**：支持PRD中描述的运动控制和状态监测功能

### **视觉数据接口**（已验证）
- **文件位置**：`/usr/local/include/unitree/ros2_idl/Image_.hpp`, `PointCloud2_.hpp`  
- **接口支持**：
  - **Image_**: 支持前置高清摄像头数据流(1280×720@120°)
  - **PointCloud2_**: 支持深度相机点云数据(Intel RealSense D435i)
- **PRD对应**：完全匹配PRD中描述的"前置高清广角摄像头"和"深度相机"系统

### **技术实现验证状态**
- ✅ **C++ SDK库**：已安装 (`/usr/local/lib/libunitree_go2_sdk.a`等)
- ✅ **API头文件**：已验证 (`/usr/local/include/unitree/robot/go2/sport/`)
- ✅ **数据结构**：所有PRD描述的硬件接口均找到对应的C++定义
- ❌ **Python SDK**：unitree_sdk2py待安装（关键缺失组件）

### **接口可用性风险评估**
- **风险等级**：🟢 **低风险** - 所有核心硬件接口已验证存在
- **主要风险**：Python绑定层缺失，需要安装unitree_sdk2py
- **缓解策略**：优先安装Python SDK，建立基础通信测试，逐步验证接口调用

## 参考技术文档  
- Unitree Go2官方文档与SDK
- ROS2 Foxy开发指南
- NVIDIA Jetson Orin NX技术规格
- Qwen2.5模型部署指南

## 性能指标要求
- **语音响应延迟**：
  - **简单指令**：< 3秒（端到端，从语音输入到动作开始）
  - **复杂指令**：< 4秒（涉及动作组合或长文本生成）
  - **性能分解**：ASR(0.3-0.5s) + LLM(1.5-2.5s) + TTS(0.1-0.2s) + 系统开销(0.1-0.2s)
- **ASR识别延迟**：< 500毫秒（Parakeet-TDT模型推理时间）
- **ASR识别准确率**：> 90%（嘈杂环境现实预期），> 95%（安静环境理想状态）
- **TTS合成延迟**：< 200毫秒（esnya/japanese_speecht5_tts推理时间）
- **TTS音质要求**：自然度评分 > 4.0/5.0（MOS评分）
- **YOLO物体检测性能**（双摄像头配置）：
  - **主摄像头检测延迟**：< 16毫秒（目标60 FPS，Unitree摄像头1280×720，YOLOv8s + TensorRT）
  - **深度摄像头检测延迟**：< 25毫秒（目标40 FPS，RealSense RGB 1920×1080，备份模式使用）
  - **双摄像头融合延迟**：< 5毫秒（检测结果空间对齐和置信度融合）
  - **主摄像头检测准确率**：> 83%（120°广角导航检测，考虑边缘畸变影响）
  - **深度摄像头检测准确率**：> 88%（高分辨率细节检测，69°视角内精确识别）
  - **融合检测综合准确率**：> 90%（多摄像头交叉验证和置信度加权融合）
  - **3D定位精度**：< 0.15米（YOLO 2D检测 + RealSense深度信息融合）
  - **摄像头切换延迟**：< 2秒（主摄像头故障时自动切换到备份的恢复时间）
  - **计算资源占用**：< 25% GPU利用率（主摄像头），额外15% GPU（备份检测启用时）
- **语义推理性能**：
  - **功能区域识别准确率**：> 80%（基于物体分布的环境类型推断）
  - **pseudo labels质量**：> 75%有效率（通过人工验证评估）
  - **语义一致性**：相同环境重复识别一致性 > 90%
- **LED状态切换延迟**：< 200毫秒（确保即时视觉反馈）
- **动作执行时间**：基本动作 < 5秒
- **LED状态同步精度**：LED状态与系统状态同步误差 < 100毫秒
- **无监督导航性能**：
  - **环境适应时间**：新环境基础理解 < 15分钟
  - **导航精度**：位置误差 < 0.5米（结合YOLO语义增强）
  - **路径优化效率**：相比随机探索提升 > 200%效率
- **连续运行时间**：> 1.5小时（考虑YOLO+LLM+导航的综合热管理）
- **环境光适应性**：LED亮度在不同光线条件下的可见性和舒适度保证
- **热管理要求**：GPU温度 < 85°C，CPU温度 < 80°C（YOLO+无监督学习持续运行状态）

## 展示场景设计
- **智能交互展示**：观众通过「ねえ、くら」唤醒与Cluadia（くら）进行自然日文语音对话
- **YOLO增强无监督导航能力展示**：
  - **实时物体识别演示**：大屏幕实时显示Cluadia通过YOLO检测到的物体（人员、桌椅、设备等），展示AI视觉能力
  - **智能语义推理展示**：现场演示机器人如何通过物体检测推断环境功能
    - 检测到桌椅+电脑 → 推断"办公区域"
    - 检测到书架+阅读桌 → 推断"图书区域" 
    - 检测到沙发+茶几 → 推断"休息区域"
  - **弱监督学习过程可视化**：展示YOLO检测结果如何作为pseudo labels指导无监督导航学习
  - **自主学习演示**：现场展示Cluadia如何通过「くら、案内して」等指令结合物体检测逐步学习和理解新环境
  - **多模态感知融合**：展示LiDAR几何感知 + YOLO语义理解 + 深度定位的协同工作
  - **探索行为进化展示**：展示从随机探索到目标导向探索的学习过程（寻找特定物体类型区域）
  - **适应性导航增强**：在动态人群环境中展示基于人员检测的智能避让和安全路径规划
- **综合学习能力展示**：
  - **从基础动作到复合动作**：展示多功能连续演示和学习能力
  - **交互式概念学习**：通过与多位观众的互动逐步完善对环境和任务的理解
- **个性化互动进化**：
  - **「ねえ、くら」唤醒响应**：展示基础交互模式
  - **「くらこい」接近行为**：展示个性化行为学习
  - **智能学习历史展示**：可视化展示当天学习成果
    - **物体识别统计**：展示检测到的物体类型和数量统计
    - **环境理解进化**：展示功能区域识别的改进过程
    - **导航策略优化**：展示路径规划效率的持续提升
    - **语义概念发展**：展示新学到的环境-语言映射关系
    - **探索地图可视化**：展示基于YOLO语义理解的环境认知地图

## 项目可扩展性说明
本PRD描述的是Cluadia智能机器人系统的**初期核心功能**和**当前规划阶段**。项目采用模块化设计理念，具备高度可扩展性：

- **功能模块化**：各子系统独立设计，便于新功能的集成
- **架构开放性**：预留扩展接口，支持未来研究方向的无缝添加
- **迭代式发展**：根据实际应用反馈和技术进展，持续优化和扩展功能
- **研究导向**：项目将持续跟踪前沿技术，探索更多创新应用可能性

**未来可能的扩展方向包括但不限于**：
- 更高级的AI推理能力
- 多机器人协作系统
- 环境交互与物体操作
- 情感识别与表达
- 个性化学习与适应

## 参考技术文档  
### 官方技术资源（✅ 主要参考）
- **Unitree官方GitHub组织**：https://github.com/unitreerobotics
  - **unitree_sdk2**：https://github.com/unitreerobotics/unitree_sdk2
    - 基于cyclonedx的核心SDK开发框架
    - 支持Go2、B2、H1、G1多平台
    - 完整的API文档和示例代码
  - **unitree_ros2**：https://github.com/unitreerobotics/unitree_ros2
    - ROS2原生集成支持包
    - cyclonedx_ws工作空间和消息定义
    - 官方example例程和最佳实践
- **官方开发指南**：
  - **Go2 SDK Development Guide**：官方完整开发指南
  - **ROS2 Services Interface Guide**：ROS2集成专门指南
  - **Unitree Support Documentation**：技术支持文档中心
- **官方媒体渠道**：
  - **Unitree Support - YouTube**：官方技术频道
  - **Unitree官方技术博客**：最新技术动态和案例
- **技术支持渠道**：
  - **GitHub Issues**：技术问题和bug报告首选渠道
  - **官方文档中心**：https://support.unitree.com/
  - **社区技术讨论**：活跃的开发者社区支持

### 第三方技术文档
- ROS2 Foxy开发指南：https://docs.ros.org/en/foxy/
- NVIDIA Jetson Orin NX技术规格
- Qwen2.5模型部署指南
- Intel RealSense D435i开发文档
</PRD> 