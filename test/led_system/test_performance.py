#!/usr/bin/env python3
"""
LEDç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•
éªŒè¯å“åº”æ—¶é—´ã€èµ„æºä½¿ç”¨ã€å¹¶å‘æ€§èƒ½ç­‰å…³é”®æŒ‡æ ‡
"""

import time
import threading
import psutil
import os
import unittest
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Tuple

from .led_test_base import LEDTestBase
from .test_config import get_led_test_config
from .data_collector import get_led_test_collector


class LEDPerformanceBenchmark(LEDTestBase):
    """LEDæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        super().setUp()
        self.config = get_led_test_config()
        self.collector = get_led_test_collector()
        self.collector.start_test_session("led_performance_benchmark",
                                         {'test_type': 'performance', 'module': 'led_system'})
        
        # è·å–åŸºçº¿èµ„æºä½¿ç”¨æƒ…å†µ
        self.baseline_memory = self._get_memory_usage()
        self.baseline_cpu = self._get_cpu_usage()
        
        print(f"ğŸ“Š åŸºçº¿èµ„æºä½¿ç”¨ - å†…å­˜: {self.baseline_memory:.2f}MB, CPU: {self.baseline_cpu:.1f}%")
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        self.collector.end_test_session("led_performance_benchmark")
        super().tearDown()
    
    def test_led_response_time_benchmark(self):
        """LEDå“åº”æ—¶é—´åŸºå‡†æµ‹è¯•"""
        self.assertLEDSystemReady()
        
        # æµ‹è¯•æ‰€æœ‰LEDæ¨¡å¼çš„å“åº”æ—¶é—´
        modes = ["wake_confirm", "processing_voice", "executing_action", "action_complete", "error_state"]
        response_times = {}
        
        for mode in modes:
            if not hasattr(self.led_system, mode):
                continue
                
            # å¤šæ¬¡æµ‹é‡å–å¹³å‡å€¼
            times = []
            for i in range(self.config.performance.performance_samples):
                start_time = time.perf_counter()
                
                try:
                    getattr(self.led_system, mode)()
                    end_time = time.perf_counter()
                    response_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    times.append(response_time)
                    
                    # è®°å½•æ¯æ¬¡æµ‹é‡
                    self.collector.record_performance_data("led_performance_benchmark", 
                                                          response_time=response_time)
                    
                except Exception as e:
                    self.collector.record_error("led_performance_benchmark", "response_time", 
                                               f"æ¨¡å¼ {mode} å“åº”æ—¶é—´æµ‹è¯•å¤±è´¥: {e}")
                    continue
                
                # çŸ­æš‚ç­‰å¾…é¿å…è¿‡äºé¢‘ç¹çš„è°ƒç”¨
                time.sleep(0.01)
            
            if times:
                avg_time = sum(times) / len(times)
                min_time = min(times)
                max_time = max(times)
                
                response_times[mode] = {
                    'average': avg_time,
                    'min': min_time,
                    'max': max_time,
                    'samples': len(times)
                }
                
                # éªŒè¯æ€§èƒ½è¦æ±‚
                self.assertLessEqual(avg_time, self.config.performance.max_response_time_ms,
                                   f"æ¨¡å¼ {mode} å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_time:.2f}ms")
                
                # è®°å½•æ€§èƒ½æŒ‡æ ‡
                self.collector.record_metric(f"{mode}_avg_response_time", avg_time, "ms",
                                            "led_performance_benchmark", "performance")
                self.collector.record_metric(f"{mode}_max_response_time", max_time, "ms",
                                            "led_performance_benchmark", "performance")
                
                print(f"âš¡ {mode}: å¹³å‡ {avg_time:.2f}ms, èŒƒå›´ {min_time:.2f}-{max_time:.2f}ms")
        
        # è®¡ç®—æ€»ä½“æ€§èƒ½ç»Ÿè®¡
        if response_times:
            all_averages = [times['average'] for times in response_times.values()]
            overall_avg = sum(all_averages) / len(all_averages)
            overall_max = max(times['max'] for times in response_times.values())
            
            self.collector.record_metric("overall_avg_response_time", overall_avg, "ms",
                                        "led_performance_benchmark", "performance")
            self.collector.record_metric("overall_max_response_time", overall_max, "ms",
                                        "led_performance_benchmark", "performance")
            
            print(f"ğŸ“Š æ€»ä½“æ€§èƒ½ - å¹³å‡: {overall_avg:.2f}ms, æœ€å¤§: {overall_max:.2f}ms")
        
        print("âœ… LEDå“åº”æ—¶é—´åŸºå‡†æµ‹è¯•é€šè¿‡")
    
    def test_resource_usage_monitoring(self):
        """èµ„æºä½¿ç”¨ç›‘æ§æµ‹è¯•"""
        self.assertLEDSystemReady()
        
        # è®°å½•åˆå§‹èµ„æºä½¿ç”¨
        initial_memory = self._get_memory_usage()
        initial_cpu = self._get_cpu_usage()
        
        resource_samples = []
        test_duration = 10.0  # ç›‘æ§10ç§’
        sample_interval = 0.5  # æ¯0.5ç§’é‡‡æ ·ä¸€æ¬¡
        
        start_time = time.time()
        
        def resource_monitor():
            """èµ„æºç›‘æ§çº¿ç¨‹"""
            while time.time() - start_time < test_duration:
                memory = self._get_memory_usage()
                cpu = self._get_cpu_usage()
                
                resource_samples.append({
                    'timestamp': time.time() - start_time,
                    'memory_mb': memory,
                    'cpu_percent': cpu,
                    'memory_delta': memory - initial_memory,
                    'cpu_delta': cpu - initial_cpu
                })
                
                # è®°å½•å®æ—¶æ•°æ®
                self.collector.record_performance_data("led_performance_benchmark",
                                                      cpu_usage=cpu, memory_usage=memory)
                
                time.sleep(sample_interval)
        
        def led_activity():
            """LEDæ´»åŠ¨çº¿ç¨‹"""
            modes = ["wake_confirm", "processing_voice", "executing_action", "action_complete"]
            
            while time.time() - start_time < test_duration:
                for mode in modes:
                    if hasattr(self.led_system, mode):
                        try:
                            getattr(self.led_system, mode)()
                            time.sleep(0.2)
                        except Exception as e:
                            self.collector.record_error("led_performance_benchmark", "resource_test",
                                                       f"LEDæ´»åŠ¨å¤±è´¥ {mode}: {e}")
                            
                        if time.time() - start_time >= test_duration:
                            break
        
        # å¹¶å‘è¿è¡Œç›‘æ§å’ŒLEDæ´»åŠ¨
        monitor_thread = threading.Thread(target=resource_monitor)
        activity_thread = threading.Thread(target=led_activity)
        
        monitor_thread.start()
        activity_thread.start()
        
        monitor_thread.join()
        activity_thread.join()
        
        # åˆ†æèµ„æºä½¿ç”¨æ•°æ®
        if resource_samples:
            memory_usage = [s['memory_mb'] for s in resource_samples]
            cpu_usage = [s['cpu_percent'] for s in resource_samples]
            memory_deltas = [s['memory_delta'] for s in resource_samples]
            
            avg_memory = sum(memory_usage) / len(memory_usage)
            max_memory = max(memory_usage)
            avg_cpu = sum(cpu_usage) / len(cpu_usage)
            max_cpu = max(cpu_usage)
            max_memory_delta = max(memory_deltas)
            
            # éªŒè¯èµ„æºä½¿ç”¨åœ¨åˆç†èŒƒå›´å†…
            self.assertLessEqual(max_memory_delta, self.config.performance.baseline_memory_mb,
                               f"å†…å­˜å¢é•¿è¿‡å¤§: {max_memory_delta:.2f}MB")
            self.assertLessEqual(avg_cpu, self.config.performance.baseline_cpu_threshold,
                               f"CPUä½¿ç”¨ç‡è¿‡é«˜: {avg_cpu:.1f}%")
            
            # è®°å½•èµ„æºä½¿ç”¨æŒ‡æ ‡
            self.collector.record_metric("avg_memory_usage", avg_memory, "MB",
                                        "led_performance_benchmark", "resource")
            self.collector.record_metric("max_memory_usage", max_memory, "MB",
                                        "led_performance_benchmark", "resource")
            self.collector.record_metric("max_memory_delta", max_memory_delta, "MB",
                                        "led_performance_benchmark", "resource")
            self.collector.record_metric("avg_cpu_usage", avg_cpu, "%",
                                        "led_performance_benchmark", "resource")
            self.collector.record_metric("max_cpu_usage", max_cpu, "%",
                                        "led_performance_benchmark", "resource")
            
            print(f"ğŸ“ˆ èµ„æºä½¿ç”¨ç»Ÿè®¡:")
            print(f"   å†…å­˜: å¹³å‡ {avg_memory:.2f}MB, æœ€å¤§ {max_memory:.2f}MB, å¢é•¿ {max_memory_delta:.2f}MB")
            print(f"   CPU: å¹³å‡ {avg_cpu:.1f}%, æœ€å¤§ {max_cpu:.1f}%")
        
        print("âœ… èµ„æºä½¿ç”¨ç›‘æ§æµ‹è¯•é€šè¿‡")
    
    def test_concurrent_performance(self):
        """å¹¶å‘æ€§èƒ½æµ‹è¯•"""
        self.assertLEDSystemReady()
        
        # å¹¶å‘çº§åˆ«æµ‹è¯•
        concurrent_levels = [1, 5, 10, 20]
        performance_results = {}
        
        for concurrent_count in concurrent_levels:
            print(f"ğŸ”„ æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrent_count}")
            
            # å‡†å¤‡ä»»åŠ¡
            def led_task(task_id: int) -> Dict[str, Any]:
                mode = ["wake_confirm", "processing_voice", "executing_action"][task_id % 3]
                
                start_time = time.perf_counter()
                try:
                    if hasattr(self.led_system, mode):
                        getattr(self.led_system, mode)()
                    end_time = time.perf_counter()
                    
                    return {
                        'task_id': task_id,
                        'mode': mode,
                        'duration': (end_time - start_time) * 1000,
                        'success': True
                    }
                except Exception as e:
                    return {
                        'task_id': task_id,
                        'mode': mode,
                        'error': str(e),
                        'success': False
                    }
            
            # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
            start_time = time.perf_counter()
            
            with ThreadPoolExecutor(max_workers=concurrent_count) as executor:
                futures = [executor.submit(led_task, i) for i in range(concurrent_count)]
                results = [future.result() for future in as_completed(futures)]
            
            end_time = time.perf_counter()
            total_duration = end_time - start_time
            
            # åˆ†æç»“æœ
            successful_tasks = [r for r in results if r['success']]
            failed_tasks = [r for r in results if not r['success']]
            
            if successful_tasks:
                durations = [r['duration'] for r in successful_tasks]
                avg_duration = sum(durations) / len(durations)
                max_duration = max(durations)
                min_duration = min(durations)
                
                success_rate = (len(successful_tasks) / len(results)) * 100
                throughput = len(successful_tasks) / total_duration  # ä»»åŠ¡/ç§’
                
                performance_results[concurrent_count] = {
                    'success_rate': success_rate,
                    'avg_duration': avg_duration,
                    'max_duration': max_duration,
                    'min_duration': min_duration,
                    'throughput': throughput,
                    'total_duration': total_duration * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
                    'failed_count': len(failed_tasks)
                }
                
                # è®°å½•å¹¶å‘æ€§èƒ½æŒ‡æ ‡
                self.collector.record_metric(f"concurrent_{concurrent_count}_success_rate", success_rate, "%",
                                            "led_performance_benchmark", "concurrent")
                self.collector.record_metric(f"concurrent_{concurrent_count}_avg_duration", avg_duration, "ms",
                                            "led_performance_benchmark", "concurrent")
                self.collector.record_metric(f"concurrent_{concurrent_count}_throughput", throughput, "ops/s",
                                            "led_performance_benchmark", "concurrent")
                
                print(f"   æˆåŠŸç‡: {success_rate:.1f}%, å¹³å‡è€—æ—¶: {avg_duration:.2f}ms, ååé‡: {throughput:.1f} ops/s")
                
                # éªŒè¯æ€§èƒ½è¦æ±‚
                self.assertGreaterEqual(success_rate, 95.0, 
                                       f"å¹¶å‘çº§åˆ« {concurrent_count} æˆåŠŸç‡è¿‡ä½: {success_rate:.1f}%")
                
                # è®°å½•å¤±è´¥çš„ä»»åŠ¡
                for failed_task in failed_tasks:
                    self.collector.record_error("led_performance_benchmark", "concurrent_task",
                                               f"ä»»åŠ¡ {failed_task['task_id']} å¤±è´¥: {failed_task.get('error', 'Unknown')}")
            
            # çŸ­æš‚ä¼‘æ¯é¿å…ç³»ç»Ÿè¿‡è½½
            time.sleep(0.5)
        
        # åˆ†æå¹¶å‘æ€§èƒ½è¶‹åŠ¿
        if len(performance_results) > 1:
            throughputs = [r['throughput'] for r in performance_results.values()]
            max_throughput = max(throughputs)
            optimal_concurrent = max(performance_results.keys(), 
                                   key=lambda k: performance_results[k]['throughput'])
            
            self.collector.record_metric("max_throughput", max_throughput, "ops/s",
                                        "led_performance_benchmark", "concurrent")
            self.collector.record_metric("optimal_concurrent_level", optimal_concurrent, "count",
                                        "led_performance_benchmark", "concurrent")
            
            print(f"ğŸ“Š å¹¶å‘æ€§èƒ½åˆ†æ - æœ€å¤§ååé‡: {max_throughput:.1f} ops/s (å¹¶å‘çº§åˆ«: {optimal_concurrent})")
        
        print("âœ… å¹¶å‘æ€§èƒ½æµ‹è¯•é€šè¿‡")
    
    def test_memory_leak_detection(self):
        """å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•"""
        self.assertLEDSystemReady()
        
        # è®°å½•åˆå§‹å†…å­˜
        initial_memory = self._get_memory_usage()
        memory_samples = [initial_memory]
        
        # æ‰§è¡Œå¤§é‡LEDæ“ä½œ
        operations_count = 1000
        sample_interval = 100  # æ¯100æ¬¡æ“ä½œé‡‡æ ·ä¸€æ¬¡
        
        modes = ["wake_confirm", "processing_voice", "executing_action", "action_complete"]
        
        for i in range(operations_count):
            mode = modes[i % len(modes)]
            
            try:
                if hasattr(self.led_system, mode):
                    getattr(self.led_system, mode)()
                
                # å®šæœŸé‡‡æ ·å†…å­˜ä½¿ç”¨
                if (i + 1) % sample_interval == 0:
                    current_memory = self._get_memory_usage()
                    memory_samples.append(current_memory)
                    
                    memory_growth = current_memory - initial_memory
                    self.collector.record_metric(f"memory_at_operation_{i+1}", current_memory, "MB",
                                                "led_performance_benchmark", "memory_leak")
                    
                    print(f"ğŸ’¾ æ“ä½œ {i+1}/{operations_count}: å†…å­˜ {current_memory:.2f}MB (å¢é•¿: {memory_growth:+.2f}MB)")
                
            except Exception as e:
                self.collector.record_error("led_performance_benchmark", "memory_leak_test",
                                           f"æ“ä½œ {i+1} å¤±è´¥: {e}")
        
        # åˆ†æå†…å­˜å¢é•¿è¶‹åŠ¿
        if len(memory_samples) >= 3:
            final_memory = memory_samples[-1]
            total_growth = final_memory - initial_memory
            
            # è®¡ç®—å†…å­˜å¢é•¿ç‡
            memory_deltas = [memory_samples[i] - memory_samples[i-1] 
                           for i in range(1, len(memory_samples))]
            avg_growth_per_sample = sum(memory_deltas) / len(memory_deltas)
            
            # æ£€æµ‹æ˜¯å¦å­˜åœ¨æ˜¾è‘—å†…å­˜æ³„æ¼
            leak_threshold = self.config.stability.memory_leak_threshold_mb
            
            self.assertLessEqual(total_growth, leak_threshold,
                               f"æ£€æµ‹åˆ°å†…å­˜æ³„æ¼: æ€»å¢é•¿ {total_growth:.2f}MB è¶…è¿‡é˜ˆå€¼ {leak_threshold}MB")
            
            # è®°å½•å†…å­˜æ³„æ¼åˆ†æç»“æœ
            self.collector.record_metric("total_memory_growth", total_growth, "MB",
                                        "led_performance_benchmark", "memory_leak")
            self.collector.record_metric("avg_growth_per_sample", avg_growth_per_sample, "MB",
                                        "led_performance_benchmark", "memory_leak")
            self.collector.record_metric("operations_tested", operations_count, "count",
                                        "led_performance_benchmark", "memory_leak")
            
            # åˆ¤æ–­å†…å­˜æ³„æ¼é£é™©çº§åˆ«
            if total_growth <= leak_threshold * 0.3:
                risk_level = "ä½"
            elif total_growth <= leak_threshold * 0.7:
                risk_level = "ä¸­"
            else:
                risk_level = "é«˜"
            
            print(f"ğŸ” å†…å­˜æ³„æ¼åˆ†æ:")
            print(f"   æ€»å†…å­˜å¢é•¿: {total_growth:.2f}MB")
            print(f"   å¹³å‡å¢é•¿ç‡: {avg_growth_per_sample:.3f}MB/sample")
            print(f"   é£é™©çº§åˆ«: {risk_level}")
        
        print("âœ… å†…å­˜æ³„æ¼æ£€æµ‹æµ‹è¯•é€šè¿‡")
    
    def test_performance_under_load(self):
        """è´Ÿè½½ä¸‹æ€§èƒ½æµ‹è¯•"""
        self.assertLEDSystemReady()
        
        # æ¨¡æ‹Ÿç³»ç»Ÿè´Ÿè½½
        def cpu_load_generator():
            """CPUè´Ÿè½½ç”Ÿæˆå™¨"""
            end_time = time.time() + 5.0  # è¿è¡Œ5ç§’
            while time.time() < end_time:
                # æ‰§è¡Œä¸€äº›CPUå¯†é›†å‹æ“ä½œ
                sum(i * i for i in range(1000))
        
        def memory_load_generator():
            """å†…å­˜è´Ÿè½½ç”Ÿæˆå™¨"""
            # åˆ†é…ä¸€äº›å†…å­˜ï¼ˆä½†ä¸è¦å¤ªå¤šå½±å“ç³»ç»Ÿï¼‰
            data = [list(range(1000)) for _ in range(100)]
            time.sleep(5.0)
            del data
        
        # åœ¨ä¸åŒè´Ÿè½½æ¡ä»¶ä¸‹æµ‹è¯•LEDæ€§èƒ½
        load_conditions = [
            ("normal", None),
            ("cpu_load", cpu_load_generator),
            ("memory_load", memory_load_generator)
        ]
        
        for condition_name, load_generator in load_conditions:
            print(f"ğŸ”„ æµ‹è¯•è´Ÿè½½æ¡ä»¶: {condition_name}")
            
            # å¯åŠ¨è´Ÿè½½ï¼ˆå¦‚æœæœ‰ï¼‰
            load_thread = None
            if load_generator:
                load_thread = threading.Thread(target=load_generator)
                load_thread.start()
            
            # æµ‹è¯•LEDæ€§èƒ½
            led_response_times = []
            test_operations = 50
            
            for i in range(test_operations):
                mode = ["wake_confirm", "processing_voice", "executing_action"][i % 3]
                
                start_time = time.perf_counter()
                try:
                    if hasattr(self.led_system, mode):
                        getattr(self.led_system, mode)()
                    end_time = time.perf_counter()
                    
                    response_time = (end_time - start_time) * 1000
                    led_response_times.append(response_time)
                    
                except Exception as e:
                    self.collector.record_error("led_performance_benchmark", "load_test",
                                               f"è´Ÿè½½ {condition_name} ä¸‹æ“ä½œå¤±è´¥: {e}")
                
                time.sleep(0.05)  # çŸ­æš‚é—´éš”
            
            # ç­‰å¾…è´Ÿè½½çº¿ç¨‹ç»“æŸ
            if load_thread:
                load_thread.join()
            
            # åˆ†æåœ¨å½“å‰è´Ÿè½½ä¸‹çš„æ€§èƒ½
            if led_response_times:
                avg_response = sum(led_response_times) / len(led_response_times)
                max_response = max(led_response_times)
                min_response = min(led_response_times)
                
                # è®°å½•è´Ÿè½½ä¸‹çš„æ€§èƒ½æŒ‡æ ‡
                self.collector.record_metric(f"response_time_under_{condition_name}_avg", avg_response, "ms",
                                            "led_performance_benchmark", "load_test")
                self.collector.record_metric(f"response_time_under_{condition_name}_max", max_response, "ms",
                                            "led_performance_benchmark", "load_test")
                
                # éªŒè¯åœ¨è´Ÿè½½ä¸‹æ€§èƒ½ä»ç„¶å¯æ¥å—
                acceptable_threshold = self.config.performance.max_response_time_ms * 1.5  # å…è®¸50%çš„æ€§èƒ½ä¸‹é™
                self.assertLessEqual(avg_response, acceptable_threshold,
                                   f"è´Ÿè½½ {condition_name} ä¸‹å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response:.2f}ms")
                
                print(f"   å¹³å‡å“åº”æ—¶é—´: {avg_response:.2f}ms, èŒƒå›´: {min_response:.2f}-{max_response:.2f}ms")
        
        print("âœ… è´Ÿè½½ä¸‹æ€§èƒ½æµ‹è¯•é€šè¿‡")
    
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            return memory_info.rss / 1024 / 1024  # è½¬æ¢ä¸ºMB
        except Exception:
            return 0.0
    
    def _get_cpu_usage(self) -> float:
        """è·å–å½“å‰CPUä½¿ç”¨ç‡ï¼ˆ%ï¼‰"""
        try:
            return psutil.cpu_percent(interval=0.1)
        except Exception:
            return 0.0


class LEDPerformanceRegression(LEDTestBase):
    """LEDæ€§èƒ½å›å½’æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        super().setUp()
        self.config = get_led_test_config()
        self.collector = get_led_test_collector()
        self.collector.start_test_session("led_performance_regression",
                                         {'test_type': 'regression', 'module': 'led_system'})
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        self.collector.end_test_session("led_performance_regression")
        super().tearDown()
    
    def test_performance_baseline_comparison(self):
        """æ€§èƒ½åŸºçº¿å¯¹æ¯”æµ‹è¯•"""
        self.assertLEDSystemReady()
        
        # å®šä¹‰æ€§èƒ½åŸºçº¿ï¼ˆè¿™äº›å€¼åº”è¯¥åŸºäºä¹‹å‰çš„æµ‹è¯•å»ºç«‹ï¼‰
        performance_baselines = {
            'wake_confirm_response_time': 50.0,      # ms
            'processing_voice_response_time': 45.0,  # ms
            'executing_action_response_time': 40.0,  # ms
            'action_complete_response_time': 55.0,   # ms
            'error_state_response_time': 30.0,       # ms
            'memory_usage_threshold': 150.0,         # MB
            'cpu_usage_threshold': 25.0              # %
        }
        
        # æµ‹è¯•å½“å‰æ€§èƒ½
        current_performance = {}
        
        modes = ["wake_confirm", "processing_voice", "executing_action", "action_complete", "error_state"]
        
        for mode in modes:
            if not hasattr(self.led_system, mode):
                continue
            
            # å¤šæ¬¡æµ‹é‡æ±‚å¹³å‡
            response_times = []
            for _ in range(20):
                start_time = time.perf_counter()
                try:
                    getattr(self.led_system, mode)()
                    end_time = time.perf_counter()
                    response_times.append((end_time - start_time) * 1000)
                except Exception as e:
                    self.collector.record_error("led_performance_regression", "baseline_test",
                                               f"æ¨¡å¼ {mode} åŸºçº¿æµ‹è¯•å¤±è´¥: {e}")
                time.sleep(0.01)
            
            if response_times:
                avg_response_time = sum(response_times) / len(response_times)
                current_performance[f"{mode}_response_time"] = avg_response_time
                
                # ä¸åŸºçº¿å¯¹æ¯”
                baseline_key = f"{mode}_response_time"
                if baseline_key in performance_baselines:
                    baseline_value = performance_baselines[baseline_key]
                    performance_ratio = avg_response_time / baseline_value
                    
                    # å…è®¸10%çš„æ€§èƒ½æ³¢åŠ¨
                    self.assertLessEqual(performance_ratio, 1.1,
                                       f"æ¨¡å¼ {mode} æ€§èƒ½å›å½’: å½“å‰ {avg_response_time:.2f}ms > åŸºçº¿ {baseline_value:.2f}ms")
                    
                    # è®°å½•æ€§èƒ½å¯¹æ¯”
                    self.collector.record_metric(f"{mode}_performance_ratio", performance_ratio, "ratio",
                                                "led_performance_regression", "comparison")
                    
                    status = "âœ…" if performance_ratio <= 1.0 else "âš ï¸" if performance_ratio <= 1.1 else "âŒ"
                    print(f"{status} {mode}: {avg_response_time:.2f}ms (åŸºçº¿: {baseline_value:.2f}ms, æ¯”ç‡: {performance_ratio:.2f})")
        
        # æ£€æŸ¥èµ„æºä½¿ç”¨
        current_memory = self._get_memory_usage()
        current_cpu = self._get_cpu_usage()
        
        current_performance['memory_usage'] = current_memory
        current_performance['cpu_usage'] = current_cpu
        
        # ä¸åŸºçº¿å¯¹æ¯”èµ„æºä½¿ç”¨
        if current_memory > performance_baselines['memory_usage_threshold']:
            print(f"âš ï¸ å†…å­˜ä½¿ç”¨è¶…è¿‡åŸºçº¿: {current_memory:.2f}MB > {performance_baselines['memory_usage_threshold']}MB")
        
        if current_cpu > performance_baselines['cpu_usage_threshold']:
            print(f"âš ï¸ CPUä½¿ç”¨è¶…è¿‡åŸºçº¿: {current_cpu:.1f}% > {performance_baselines['cpu_usage_threshold']}%")
        
        # è®°å½•å®Œæ•´çš„æ€§èƒ½æŠ¥å‘Š
        self.collector.record_metric("performance_baseline_check", "completed", "status",
                                    "led_performance_regression", "regression")
        
        print("âœ… æ€§èƒ½åŸºçº¿å¯¹æ¯”æµ‹è¯•å®Œæˆ")
    
    def _get_memory_usage(self) -> float:
        """è·å–å½“å‰è¿›ç¨‹å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            return memory_info.rss / 1024 / 1024
        except Exception:
            return 0.0
    
    def _get_cpu_usage(self) -> float:
        """è·å–å½“å‰CPUä½¿ç”¨ç‡ï¼ˆ%ï¼‰"""
        try:
            return psutil.cpu_percent(interval=0.1)
        except Exception:
            return 0.0


if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    suite = unittest.TestSuite()
    
    # æ·»åŠ åŸºå‡†æµ‹è¯•
    suite.addTest(unittest.makeSuite(LEDPerformanceBenchmark))
    
    # æ·»åŠ å›å½’æµ‹è¯•
    suite.addTest(unittest.makeSuite(LEDPerformanceRegression))
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # ç”ŸæˆæŠ¥å‘Š
    collector = get_led_test_collector()
    collector.save_data("led_performance_test_results")
    
    print(f"\n{'='*60}")
    print(f"LEDæ€§èƒ½æµ‹è¯•å®Œæˆ - æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}, "
          f"å¤±è´¥: {len(result.failures)}, é”™è¯¯: {len(result.errors)}")
    print(f"{'='*60}") 