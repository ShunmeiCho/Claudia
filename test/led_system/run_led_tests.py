 #!/usr/bin/env python3
"""
LEDæ§åˆ¶ç³»ç»Ÿæµ‹è¯•æ¡†æ¶ä¸»è¿è¡Œå™¨
ä»»åŠ¡6.5: å…¨é¢æµ‹è¯•ã€éªŒè¯å’Œæ€§èƒ½ä¼˜åŒ–

æä¾›ç»Ÿä¸€çš„æµ‹è¯•æ‰§è¡Œã€æŠ¥å‘Šç”Ÿæˆå’Œç»“æœåˆ†æ
"""

import os
import sys
import time
import argparse
import unittest
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from .test_config import get_led_test_config, reset_led_test_config
from .data_collector import get_led_test_collector, reset_led_test_collector
from .test_led_modes import LEDModesFunctionalTest, LEDModesStressTest
from .test_performance import LEDPerformanceBenchmark, LEDPerformanceRegression


class LEDTestRunner:
    """LEDæµ‹è¯•æ¡†æ¶ä¸»è¿è¡Œå™¨"""
    
    def __init__(self, config_overrides: Optional[Dict[str, Any]] = None):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        self.config = get_led_test_config()
        self.collector = get_led_test_collector()
        
        # åº”ç”¨é…ç½®è¦†ç›–
        if config_overrides:
            self._apply_config_overrides(config_overrides)
        
        self.test_suites = {
            'functional': self._create_functional_suite,
            'performance': self._create_performance_suite,
            'stress': self._create_stress_suite,
            'regression': self._create_regression_suite,
            'all': self._create_all_suites
        }
        
        print(f"ğŸ¤– LEDæ§åˆ¶ç³»ç»Ÿæµ‹è¯•æ¡†æ¶ v1.0")
        print(f"ğŸ“… åˆå§‹åŒ–æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"âš™ï¸ æµ‹è¯•æ¨¡å¼: {self.config.get_test_mode()}")
    
    def _apply_config_overrides(self, overrides: Dict[str, Any]):
        """åº”ç”¨é…ç½®è¦†ç›–"""
        for key, value in overrides.items():
            if hasattr(self.config, key):
                setattr(self.config, key, value)
                print(f"ğŸ”§ é…ç½®è¦†ç›–: {key} = {value}")
    
    def run_tests(self, test_type: str = "all", verbosity: int = 2, 
                 output_dir: Optional[str] = None) -> Dict[str, Any]:
        """è¿è¡ŒæŒ‡å®šç±»å‹çš„æµ‹è¯•"""
        
        print(f"\n{'='*80}")
        print(f"ğŸš€ å¼€å§‹æ‰§è¡Œ {test_type.upper()} æµ‹è¯•")
        print(f"{'='*80}")
        
        # åˆ›å»ºæµ‹è¯•å¥—ä»¶
        if test_type not in self.test_suites:
            raise ValueError(f"ä¸æ”¯æŒçš„æµ‹è¯•ç±»å‹: {test_type}. æ”¯æŒçš„ç±»å‹: {list(self.test_suites.keys())}")
        
        suite_creator = self.test_suites[test_type]
        
        if test_type == 'all':
            # è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶
            results = {}
            for suite_name in ['functional', 'performance', 'stress', 'regression']:
                if suite_name == 'stress' and not self.config.is_stress_test_enabled():
                    print(f"â­ï¸ è·³è¿‡ {suite_name} æµ‹è¯•ï¼ˆæœªå¯ç”¨å‹åŠ›æµ‹è¯•ï¼‰")
                    continue
                
                print(f"\nğŸ“‹ è¿è¡Œ {suite_name.upper()} æµ‹è¯•å¥—ä»¶...")
                suite = self.test_suites[suite_name]()
                result = self._run_test_suite(suite, verbosity)
                results[suite_name] = result
            
            # åˆå¹¶æ‰€æœ‰ç»“æœ
            combined_result = self._combine_results(results)
            
        else:
            # è¿è¡Œå•ä¸ªæµ‹è¯•å¥—ä»¶
            if test_type == 'stress' and not self.config.is_stress_test_enabled():
                print("âš ï¸ å‹åŠ›æµ‹è¯•å·²ç¦ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®")
                return {'error': 'stress_tests_disabled'}
            
            suite = suite_creator()
            combined_result = self._run_test_suite(suite, verbosity)
        
        # ç”ŸæˆæŠ¥å‘Š
        self._generate_test_reports(output_dir)
        
        # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
        self._display_test_summary(combined_result)
        
        return combined_result
    
    def _create_functional_suite(self) -> unittest.TestSuite:
        """åˆ›å»ºåŠŸèƒ½æµ‹è¯•å¥—ä»¶"""
        suite = unittest.TestSuite()
        suite.addTest(unittest.makeSuite(LEDModesFunctionalTest))
        return suite
    
    def _create_performance_suite(self) -> unittest.TestSuite:
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•å¥—ä»¶"""
        suite = unittest.TestSuite()
        suite.addTest(unittest.makeSuite(LEDPerformanceBenchmark))
        return suite
    
    def _create_stress_suite(self) -> unittest.TestSuite:
        """åˆ›å»ºå‹åŠ›æµ‹è¯•å¥—ä»¶"""
        suite = unittest.TestSuite()
        suite.addTest(unittest.makeSuite(LEDModesStressTest))
        return suite
    
    def _create_regression_suite(self) -> unittest.TestSuite:
        """åˆ›å»ºå›å½’æµ‹è¯•å¥—ä»¶"""
        suite = unittest.TestSuite()
        suite.addTest(unittest.makeSuite(LEDPerformanceRegression))
        return suite
    
    def _create_all_suites(self) -> unittest.TestSuite:
        """åˆ›å»ºæ‰€æœ‰æµ‹è¯•å¥—ä»¶"""
        # è¿™ä¸ªæ–¹æ³•ä¸ä¼šè¢«ç›´æ¥è°ƒç”¨ï¼Œå› ä¸º 'all' ç±»å‹åœ¨ run_tests ä¸­ç‰¹æ®Šå¤„ç†
        return unittest.TestSuite()
    
    def _run_test_suite(self, suite: unittest.TestSuite, verbosity: int) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•å¥—ä»¶å¹¶è¿”å›ç»“æœ"""
        start_time = time.time()
        
        # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
        runner = unittest.TextTestRunner(
            verbosity=verbosity,
            stream=sys.stdout,
            buffer=False
        )
        
        # è¿è¡Œæµ‹è¯•
        result = runner.run(suite)
        
        end_time = time.time()
        duration = end_time - start_time
        
        # ç»Ÿè®¡ç»“æœ
        test_results = {
            'tests_run': result.testsRun,
            'failures': len(result.failures),
            'errors': len(result.errors),
            'skipped': len(result.skipped) if hasattr(result, 'skipped') else 0,
            'success_count': result.testsRun - len(result.failures) - len(result.errors),
            'success_rate': ((result.testsRun - len(result.failures) - len(result.errors)) / result.testsRun * 100) if result.testsRun > 0 else 0,
            'duration': duration,
            'failure_details': [{'test': str(test), 'traceback': traceback} for test, traceback in result.failures],
            'error_details': [{'test': str(test), 'traceback': traceback} for test, traceback in result.errors]
        }
        
        return test_results
    
    def _combine_results(self, results: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """åˆå¹¶å¤šä¸ªæµ‹è¯•ç»“æœ"""
        combined = {
            'tests_run': 0,
            'failures': 0,
            'errors': 0,
            'skipped': 0,
            'success_count': 0,
            'total_duration': 0,
            'suite_results': results,
            'failure_details': [],
            'error_details': []
        }
        
        for suite_name, result in results.items():
            if 'error' in result:
                continue
                
            combined['tests_run'] += result['tests_run']
            combined['failures'] += result['failures']
            combined['errors'] += result['errors']
            combined['skipped'] += result['skipped']
            combined['success_count'] += result['success_count']
            combined['total_duration'] += result['duration']
            combined['failure_details'].extend(result['failure_details'])
            combined['error_details'].extend(result['error_details'])
        
        combined['success_rate'] = (combined['success_count'] / combined['tests_run'] * 100) if combined['tests_run'] > 0 else 0
        
        return combined
    
    def _generate_test_reports(self, output_dir: Optional[str] = None):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        if output_dir:
            self.collector.output_dir = Path(output_dir)
        
        # ç”ŸæˆHTMLæŠ¥å‘Š
        html_report = self.collector.generate_report()
        
        # ä¿å­˜JSONæ•°æ®
        json_data = self.collector.save_data(format_type="json")
        
        # ä¿å­˜CSVæ•°æ®
        csv_data = self.collector.save_data(format_type="csv")
        
        print(f"\nğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆ:")
        print(f"   ğŸ“‹ HTMLæŠ¥å‘Š: {html_report}")
        print(f"   ğŸ“„ JSONæ•°æ®: {json_data}")
        print(f"   ğŸ“ˆ CSVæ•°æ®: {csv_data}")
    
    def _display_test_summary(self, results: Dict[str, Any]):
        """æ˜¾ç¤ºæµ‹è¯•æ€»ç»“"""
        print(f"\n{'='*80}")
        print(f"ğŸ“Š LEDæ§åˆ¶ç³»ç»Ÿæµ‹è¯•æ€»ç»“")
        print(f"{'='*80}")
        
        if 'error' in results:
            print(f"âŒ æµ‹è¯•æ‰§è¡Œé”™è¯¯: {results['error']}")
            return
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"ğŸ§ª æ€»æµ‹è¯•æ•°: {results['tests_run']}")
        print(f"âœ… æˆåŠŸ: {results['success_count']}")
        print(f"âŒ å¤±è´¥: {results['failures']}")
        print(f"ğŸ”¥ é”™è¯¯: {results['errors']}")
        print(f"â­ï¸ è·³è¿‡: {results['skipped']}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {results['success_rate']:.1f}%")
        print(f"â±ï¸ æ€»è€—æ—¶: {results['total_duration']:.2f}ç§’")
        
        # çŠ¶æ€åˆ¤å®š
        if results['success_rate'] >= 95:
            status_icon = "ğŸ‰"
            status_text = "ä¼˜ç§€"
        elif results['success_rate'] >= 90:
            status_icon = "âœ…"
            status_text = "è‰¯å¥½"
        elif results['success_rate'] >= 80:
            status_icon = "âš ï¸"
            status_text = "ä¸€èˆ¬"
        else:
            status_icon = "âŒ"
            status_text = "éœ€è¦æ”¹è¿›"
        
        print(f"\n{status_icon} æ€»ä½“è¯„ä¼°: {status_text}")
        
        # å¥—ä»¶è¯¦æƒ…ï¼ˆå¦‚æœæœ‰å¤šä¸ªå¥—ä»¶ï¼‰
        if 'suite_results' in results:
            print(f"\nğŸ“‹ åˆ†å¥—ä»¶ç»“æœ:")
            for suite_name, suite_result in results['suite_results'].items():
                if 'error' in suite_result:
                    print(f"   {suite_name}: âŒ {suite_result['error']}")
                else:
                    rate = suite_result['success_rate']
                    icon = "âœ…" if rate >= 95 else "âš ï¸" if rate >= 80 else "âŒ"
                    print(f"   {suite_name}: {icon} {suite_result['success_count']}/{suite_result['tests_run']} ({rate:.1f}%)")
        
        # å®æ—¶ç»Ÿè®¡
        real_time_stats = self.collector.get_real_time_stats()
        if real_time_stats:
            print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
            if 'response_time' in real_time_stats:
                rt = real_time_stats['response_time']
                print(f"   å“åº”æ—¶é—´: å¹³å‡ {rt.get('overall_avg', 0):.2f}ms, æœ€å¤§ {rt.get('overall_max', 0):.2f}ms")
            
            if 'cpu_usage' in real_time_stats:
                cpu = real_time_stats['cpu_usage']
                print(f"   CPUä½¿ç”¨: å¹³å‡ {cpu.get('avg', 0):.1f}%, æœ€å¤§ {cpu.get('max', 0):.1f}%")
            
            if 'memory_usage' in real_time_stats:
                mem = real_time_stats['memory_usage']
                print(f"   å†…å­˜ä½¿ç”¨: å¹³å‡ {mem.get('avg', 0):.1f}MB, æœ€å¤§ {mem.get('max', 0):.1f}MB")
        
        # å¤±è´¥è¯¦æƒ…ï¼ˆå¦‚æœæœ‰ï¼‰
        if results['failures'] > 0 or results['errors'] > 0:
            print(f"\nğŸ” å¤±è´¥è¯¦æƒ…:")
            for failure in results['failure_details'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"   âŒ {failure['test']}")
            
            for error in results['error_details'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                print(f"   ğŸ”¥ {error['test']}")
            
            if len(results['failure_details']) + len(results['error_details']) > 6:
                print(f"   ... æ›´å¤šè¯¦æƒ…è¯·æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š")
        
        print(f"\n{'='*80}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="LEDæ§åˆ¶ç³»ç»Ÿæµ‹è¯•æ¡†æ¶")
    
    parser.add_argument('--type', '-t', 
                       choices=['functional', 'performance', 'stress', 'regression', 'all'],
                       default='all',
                       help='æµ‹è¯•ç±»å‹ (é»˜è®¤: all)')
    
    parser.add_argument('--verbosity', '-v',
                       type=int, choices=[0, 1, 2], default=2,
                       help='è¯¦ç»†ç¨‹åº¦ (0=é™é»˜, 1=æ­£å¸¸, 2=è¯¦ç»†)')
    
    parser.add_argument('--output', '-o',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: logs/led_tests)')
    
    parser.add_argument('--hardware', 
                       action='store_true',
                       help='å¯ç”¨ç¡¬ä»¶æµ‹è¯•æ¨¡å¼')
    
    parser.add_argument('--stress',
                       action='store_true', 
                       help='å¯ç”¨å‹åŠ›æµ‹è¯•')
    
    parser.add_argument('--performance-samples',
                       type=int, default=50,
                       help='æ€§èƒ½æµ‹è¯•é‡‡æ ·æ•°é‡ (é»˜è®¤: 50)')
    
    parser.add_argument('--max-response-time',
                       type=float, default=200.0,
                       help='æœ€å¤§å“åº”æ—¶é—´é˜ˆå€¼(ms) (é»˜è®¤: 200.0)')
    
    args = parser.parse_args()
    
    # å‡†å¤‡é…ç½®è¦†ç›–
    config_overrides = {}
    
    if args.hardware:
        config_overrides['hardware.hardware_required'] = True
        config_overrides['hardware.mock_hardware'] = False
    
    if args.stress:
        config_overrides['stress_tests_enabled'] = True
    
    if args.performance_samples != 50:
        config_overrides['performance.performance_samples'] = args.performance_samples
    
    if args.max_response_time != 200.0:
        config_overrides['performance.max_response_time_ms'] = args.max_response_time
    
    try:
        # é‡ç½®æ”¶é›†å™¨ç¡®ä¿å¹²å‡€çš„æµ‹è¯•ç¯å¢ƒ
        reset_led_test_collector()
        
        # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
        runner = LEDTestRunner(config_overrides)
        
        # è¿è¡Œæµ‹è¯•
        results = runner.run_tests(
            test_type=args.type,
            verbosity=args.verbosity,
            output_dir=args.output
        )
        
        # æ ¹æ®ç»“æœè®¾ç½®é€€å‡ºç 
        if 'error' in results:
            sys.exit(1)
        elif results['failures'] > 0 or results['errors'] > 0:
            sys.exit(1)
        else:
            sys.exit(0)
            
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(130)
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•æ¡†æ¶é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()